{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu-cu12\n",
    "!pip install -U langchain-community\n",
    "!pip install langchain_openai\n",
    "!pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer, normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import joblib\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "\n",
    "# Download NLTK data for tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE_WEIGHT = 1.0\n",
    "KEYWORD_WEIGHT = 2.0\n",
    "VOTE_WEIGHT = 0.3\n",
    "ADULT_WEIGHT = 0.2\n",
    "SUMMARY_WEIGHT = 0.5\n",
    "ATMOSPHERE_WEIGHT = 0.5\n",
    "NARRATIVE_WEIGHT = 0.5\n",
    "EMOTION_WEIGHT = 0.5\n",
    "THEME_WEIGHT = 0.5\n",
    "CHARACTER_WEIGHT = 0.5\n",
    "PACING_WEIGHT = 0.5\n",
    "\n",
    "INITIAL_DATA_PATH = '/content/drive/My Drive/colab.pkl'\n",
    "BEST_MODEL_PATH = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved'\n",
    "MODEL_INFO_PATH = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved/model_info.pkl'\n",
    "NON_CLASSIFIED_EMBEDDINGS_PATH = '/content/drive/My Drive/movie_similarity/non_classified_embeddings.pkl'\n",
    "CLASSIFIED_EMBEDDINGS_PATH = '/content/drive/My Drive/movie_similarity/classified_embeddings.pkl'\n",
    "TRAINING_DATA_PATH = '/content/drive/My Drive/movie_similarity/training_data.pkl'\n",
    "TRAINING_DATA_CLEANED_PATH = '/content/drive/My Drive/movie_similarity/training_data_cleaned.pkl'\n",
    "SBERT_MODEL = 'all-MiniLM-L6-v2'\n",
    "ZSC_MODEL = 'FacebookAI/roberta-base'\n",
    "\n",
    "atmosphere_labels = [\n",
    "    'dark', 'tense', 'neutral',\n",
    "    'melancholic', 'cheerful', 'mysterious', 'foreboding',\n",
    "    'dreamy', 'chaotic', 'nostalgic', 'surreal',\n",
    "]\n",
    "\n",
    "narrative_labels = ['linear', 'non-linear', 'episodic', 'circular']\n",
    "\n",
    "theme_candidates = [\n",
    "    'redemption', 'betrayal', 'love', 'identity', 'family',\n",
    "    'loss', 'justice', 'freedom', 'corruption', 'hope',\n",
    "    'loyalty', 'power', 'isolation', 'transformation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_classified_embeddings(df):\n",
    "    df['genres'] = df['genres'].apply(lambda x: [item.strip() for item in x.split(',')] if x else [])\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: [item.strip() for item in x.split(',')] if x else [])\n",
    "    df['overview'] = df['overview'].fillna('')\n",
    "\n",
    "    df['vote_average'] = pd.to_numeric(df['vote_average'], errors='coerce')\n",
    "    df['vote_average'] = df['vote_average'].fillna(df['vote_average'].mean())\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    vote_average_scaled = scaler.fit_transform(df[['vote_average']]).flatten()\n",
    "\n",
    "    model = SentenceTransformer(SBERT_MODEL)\n",
    "\n",
    "    genre_texts = [' '.join(genres) for genres in df['genres']]\n",
    "    keyword_texts = [' '.join(keywords) for keywords in df['keywords']]\n",
    "\n",
    "    all_texts = genre_texts + keyword_texts + df['overview'].tolist()\n",
    "    all_embeddings = model.encode(all_texts, show_progress_bar=True).astype('float32')\n",
    "\n",
    "    n_genres = len(genre_texts)\n",
    "    n_keywords = len(keyword_texts)\n",
    "    genres_emb = all_embeddings[:n_genres]\n",
    "    keywords_emb = all_embeddings[n_genres:n_genres+n_keywords]\n",
    "    summary_embeddings = all_embeddings[n_genres+n_keywords:]\n",
    "\n",
    "    final_df = pd.DataFrame({\n",
    "        'id': df['id'],\n",
    "        'overview_emb': list(summary_embeddings),\n",
    "        'genres_emb': list(genres_emb),\n",
    "        'keywords_emb': list(keywords_emb),\n",
    "        'vote_average_scaled': vote_average_scaled\n",
    "    })\n",
    "\n",
    "    print(final_df.head())\n",
    "\n",
    "    final_df.to_pickle(NON_CLASSIFIED_EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.read_pickle(INITIAL_DATA_PATH)\n",
    "\n",
    "print(initial_df.head())\n",
    "\n",
    "create_non_classified_embeddings(initial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain, SimpleSequentialChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, FileChatMessageHistory, ConversationSummaryBufferMemory\n",
    "from langchain.agents import AgentType, initialize_agent, Tool\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    google_api_key=\"your-gemini-api-key\",\n",
    "    model=\"gemini-2.0-flash\", # would be better to use gemini 2.5 flash, this model sometimes generates labels that are not specified\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "def create_labeled_data(model, df):\n",
    "    system_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\n",
    "        f\"\"\"\n",
    "        You are a highly analytical and precise movie review labeler. Your job is to determine the movie's features based *only* on the provided user review. You will classify the movie features into specific categories. You are given very specific instructions for this and must not deviate from the categories or their definitions.\n",
    "\n",
    "        The possible labels for atmosphere: {atmosphere_labels}. **YOU MUST CHOOSE ONLY FROM THIS LIST.**\n",
    "        The possible labels for narrative_structure: {narrative_labels}. **YOU MUST CHOOSE ONLY FROM THIS LIST.**\n",
    "        The possible labels for themes: {theme_candidates}. **YOU MUST CHOOSE ONLY FROM THIS LIST.**\n",
    "\n",
    "         **Instructions for Themes:**\n",
    "        - Only include a theme if it is strongly and unambiguously expressed or implied by the review.\n",
    "        - Be particularly careful with 'hope' and 'redemption'. These should only be applied if there is a clear, sustained sense of optimism or a definitive moral turnaround, respectively. If a film introduces hope only to crush it, or shows no clear path to absolution, do NOT label it with 'hope' or 'redemption'.\n",
    "\n",
    "        **CRITICAL RULE: ALL LABELS YOU PROVIDE MUST BE EXACTLY MATCHED FROM THE RESPECTIVE ALLOWED LISTS. DO NOT INVENT NEW LABELS. IF NO APPLICABLE LABEL FROM THE LISTS IS FOUND FOR A CATEGORY, PROVIDE AN EMPTY LIST [] FOR THAT CATEGORY.**\n",
    "\n",
    "        The output format of your answer should be: ATMOSPHERE: (list of labels you determined) ; NARRATIVE_STRUCTURE: (list of labels you determined) ; THEMES: (list of labels you determined)\n",
    "\n",
    "        Query: {{query}}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    batch_size = 4\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "\n",
    "    chain = LLMChain(llm=model, prompt=system_prompt)\n",
    "\n",
    "    processing_df = df.copy().reset_index(drop=True)\n",
    "\n",
    "    if processing_df.empty:\n",
    "        print(\"Warning: Input DataFrame is empty. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['movie_id', 'review', 'atmosphere', 'narrative_structure', 'themes'])\n",
    "\n",
    "    print(f\"Processing {len(processing_df)} reviews in batches of {batch_size}...\")\n",
    "    print(processing_df.head())\n",
    "\n",
    "    processing_df['review'] = processing_df['review'].fillna('').astype(str)\n",
    "\n",
    "    processing_df['atmosphere'] = None\n",
    "    processing_df['narrative_structure'] = None\n",
    "    processing_df['themes'] = None\n",
    "\n",
    "    list_content_regex = re.compile(r\"\\[\\s*([^\\]]*?)\\s*\\]\")\n",
    "\n",
    "    def parse_labels_from_llm_output(text: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "        atmosphere = []\n",
    "        narrative_structure = []\n",
    "        themes = []\n",
    "\n",
    "        try:\n",
    "            atmosphere_match = re.search(r'ATMOSPHERE:\\s*\\[(.*?)\\]', text)\n",
    "            if atmosphere_match:\n",
    "                atmosphere_str = atmosphere_match.group(1)\n",
    "                atmosphere = [label.strip().strip(\"'\\\"\") for label in atmosphere_str.split(',') if label.strip()]\n",
    "\n",
    "            narrative_match = re.search(r'NARRATIVE_STRUCTURE:\\s*\\[(.*?)\\]', text)\n",
    "            if narrative_match:\n",
    "                narrative_str = narrative_match.group(1)\n",
    "                narrative_structure = [label.strip().strip(\"'\\\"\") for label in narrative_str.split(',') if label.strip()]\n",
    "\n",
    "            themes_match = re.search(r'THEMES:\\s*\\[(.*?)\\]', text)\n",
    "            if themes_match:\n",
    "                themes_str = themes_match.group(1)\n",
    "                themes = [label.strip().strip(\"'\\\"\") for label in themes_str.split(',') if label.strip()]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing labels from LLM output: {e}\")\n",
    "            print(f\"Raw LLM output causing error: {text[:200]}...\")\n",
    "            return [], [], []\n",
    "\n",
    "        return atmosphere, narrative_structure, themes\n",
    "\n",
    "    total_reviews = len(processing_df)\n",
    "\n",
    "    for i in range(0, total_reviews, batch_size):\n",
    "        batch_start_time = time.time()\n",
    "        batch_end_index = min(i + batch_size, total_reviews)\n",
    "        current_batch_df = processing_df.iloc[i:batch_end_index]\n",
    "\n",
    "        inputs_for_current_batch = [{\"query\": review_text} for review_text in current_batch_df['review'].tolist()]\n",
    "\n",
    "        print(f\"\\nProcessing batch {i // batch_size + 1}/{(total_reviews + batch_size - 1) // batch_size} (Reviews {i}-{batch_end_index-1})\")\n",
    "\n",
    "        retries = 0\n",
    "        batch_labeled_results = []\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                batch_labeled_results = chain.batch(inputs_for_current_batch)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                current_delay = retry_delay * (2 ** (retries - 1))\n",
    "                print(f\"Error during LLM batch call (Batch {i // batch_size + 1}): {e}\")\n",
    "                print(f\"Retrying in {current_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(current_delay)\n",
    "\n",
    "        if not batch_labeled_results:\n",
    "            print(f\"Failed to get results for batch {i // batch_size + 1} after {max_retries} retries. Skipping this batch.\")\n",
    "            for j in range(len(current_batch_df)):\n",
    "                row_index = i + j\n",
    "                processing_df.at[row_index, 'atmosphere'] = []\n",
    "                processing_df.at[row_index, 'narrative_structure'] = []\n",
    "                processing_df.at[row_index, 'themes'] = []\n",
    "            continue\n",
    "\n",
    "        for j, result in enumerate(batch_labeled_results):\n",
    "            row_index = i + j\n",
    "\n",
    "            if isinstance(result, dict):\n",
    "                result_text = result.get('text', str(result))\n",
    "            else:\n",
    "                result_text = str(result)\n",
    "\n",
    "            atmosphere, narrative_structure, themes = parse_labels_from_llm_output(result_text)\n",
    "\n",
    "            processing_df.at[row_index, 'atmosphere'] = atmosphere\n",
    "            processing_df.at[row_index, 'narrative_structure'] = narrative_structure\n",
    "            processing_df.at[row_index, 'themes'] = themes\n",
    "\n",
    "        batch_elapsed_time = time.time() - batch_start_time\n",
    "        print(f\"Batch {i // batch_size + 1} completed in {batch_elapsed_time:.2f} seconds.\")\n",
    "        if batch_elapsed_time < 1.0:\n",
    "            time.sleep(1.0 - batch_elapsed_time)\n",
    "\n",
    "    print(f\"\\nLabeling complete. Successfully processed {len(processing_df)} reviews\")\n",
    "    print(processing_df[['id', 'review', 'atmosphere', 'narrative_structure', 'themes']].head())\n",
    "\n",
    "    try:\n",
    "        print(processing_df[['atmosphere', 'narrative_structure', 'themes']].apply(lambda col: col.explode().value_counts()).T.fillna(0).astype(int))\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying label counts: {e}\")\n",
    "        print(\"Label assignment completed successfully\")\n",
    "\n",
    "    processing_df.to_pickle(TRAINING_DATA_PATH)\n",
    "\n",
    "    return processing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.read_pickle(INITIAL_DATA_PATH)\n",
    "\n",
    "print(initial_df.head())\n",
    "\n",
    "create_labeled_data(gemini, initial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_pickle(TRAINING_DATA_PATH)\n",
    "\n",
    "expected_labels = {\n",
    "    'atmosphere': set(atmosphere_labels),\n",
    "    'narrative_structure': set(narrative_labels),\n",
    "    'themes': set(theme_candidates)\n",
    "}\n",
    "\n",
    "def clean_labels(value, valid_labels):\n",
    "    if value is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if pd.isna(value):\n",
    "            return None\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "\n",
    "    if isinstance(value, str):\n",
    "        labels = [label.strip() for label in value.split(',') if label.strip()]\n",
    "    elif isinstance(value, (list, np.ndarray)):\n",
    "        labels = [str(item).strip() for item in value if str(item).strip()]\n",
    "    else:\n",
    "        labels = [str(value).strip()]\n",
    "\n",
    "    clean_labels = [label for label in labels if label in valid_labels]\n",
    "\n",
    "    if clean_labels:\n",
    "        return ', '.join(clean_labels)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "clean_df = training_df.copy()\n",
    "\n",
    "for col in ['atmosphere', 'narrative_structure', 'themes']:\n",
    "    if col in clean_df.columns:\n",
    "        print(f\"Cleaning {col}...\")\n",
    "        before_count = clean_df[col].notna().sum()\n",
    "\n",
    "        clean_df[col] = clean_df[col].apply(lambda x: clean_labels(x, expected_labels[col]))\n",
    "\n",
    "        after_count = clean_df[col].notna().sum()\n",
    "        print(f\"Before: {before_count} non-null entries\")\n",
    "        print(f\"After: {after_count} non-null entries\")\n",
    "        print(f\"Removed: {before_count - after_count} entries\")\n",
    "\n",
    "label_columns = ['atmosphere', 'narrative_structure', 'themes']\n",
    "existing_label_cols = [col for col in label_columns if col in clean_df.columns]\n",
    "\n",
    "if existing_label_cols:\n",
    "    before_rows = len(clean_df)\n",
    "    clean_df = clean_df.dropna(subset=existing_label_cols, how='all')\n",
    "    after_rows = len(clean_df)\n",
    "    print(f\"\\nRemoved {before_rows - after_rows} rows with no valid labels\")\n",
    "\n",
    "print(f\"\\nFinal DataFrame shape: {clean_df.shape}\")\n",
    "print(f\"Original DataFrame shape: {initial_df.shape}\")\n",
    "\n",
    "clean_df.to_pickle(TRAINING_DATA_CLEANED_PATH)\n",
    "print(\"\\nCleaned DataFrame saved.\")\n",
    "\n",
    "print(\"\\nFinal unique labels:\")\n",
    "for col in existing_label_cols:\n",
    "    all_labels = set()\n",
    "    for value in clean_df[col].dropna():\n",
    "        if isinstance(value, str):\n",
    "            labels = [label.strip() for label in value.split(',') if label.strip()]\n",
    "            all_labels.update(labels)\n",
    "    print(f\"{col}: {sorted(list(all_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mislabels = 0\n",
    "\n",
    "for col in ['atmosphere', 'narrative_structure', 'themes']:\n",
    "    if col in clean_df.columns:\n",
    "        print(f\"\\n{col.upper()}\")\n",
    "        mislabel_count = 0\n",
    "        mislabel_examples = []\n",
    "\n",
    "        for idx, value in clean_df[col].dropna().items():\n",
    "            if isinstance(value, str):\n",
    "                labels = [label.strip() for label in value.split(',') if label.strip()]\n",
    "            elif isinstance(value, (list, np.ndarray)):\n",
    "                labels = [str(item).strip() for item in value if str(item).strip()]\n",
    "            else:\n",
    "                labels = [str(value).strip()]\n",
    "\n",
    "            bad_labels = [label for label in labels if label not in expected_labels[col]]\n",
    "            if bad_labels:\n",
    "                mislabel_count += 1\n",
    "                mislabel_examples.append((idx, bad_labels, labels))\n",
    "\n",
    "        print(f\"Bad entries: {mislabel_count}\")\n",
    "        print(f\"Bad labels found: {set([label for _, bad_labels, _ in mislabel_examples for label in bad_labels])}\")\n",
    "\n",
    "        # Show some examples\n",
    "        if mislabel_examples:\n",
    "            print(\"Examples of bad entries:\")\n",
    "            for idx, bad_labels, all_labels in mislabel_examples[:3]:\n",
    "                print(f\"Row {idx}: {all_labels} (bad: {bad_labels})\")\n",
    "\n",
    "        total_mislabels += total_mislabels\n",
    "\n",
    "print(f\"\\nTotal bad entries across all columns: {total_mislabels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = clean_df[clean_df['id'] == 13].copy()\n",
    "\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)        # Show all rows (for the 100 samples)\n",
    "pd.set_option('display.max_columns', None)     # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)    # Show full content of each cell (important for reviews)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "random_samples_df = clean_df.sample(n=100, random_state=42)\n",
    "\n",
    "print(random_samples_df[['id', 'review', 'atmosphere', 'narrative_structure', 'themes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from sklearn.metrics import classification_report, hamming_loss, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, pos_weights):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weights = pos_weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weights\n",
    "        )\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length=256):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.reviews[idx])\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            review,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(labels)\n",
    "        }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_fn=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.loss_fn is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def load_and_merge_data(path1, path2):\n",
    "    try:\n",
    "        df1 = pd.read_pickle(path1)\n",
    "        print(f\"Dataset 1 shape: {df1.shape}\")\n",
    "\n",
    "        try:\n",
    "            df2 = pd.read_pickle(path2)\n",
    "            print(f\"Dataset 2 shape: {df2.shape}\")\n",
    "            if set(df1.columns) != set(df2.columns):\n",
    "                common_cols = list(set(df1.columns) & set(df2.columns))\n",
    "                df1 = df1[common_cols]\n",
    "                df2 = df2[common_cols]\n",
    "            combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"Second dataset not found, using only the first dataset\")\n",
    "            combined_df = df1\n",
    "\n",
    "        if 'review' in combined_df.columns:\n",
    "            initial_len = len(combined_df)\n",
    "            combined_df = combined_df.drop_duplicates(subset=['review'], keep='first')\n",
    "            final_len = len(combined_df)\n",
    "            if initial_len != final_len:\n",
    "                print(f\"Removed {initial_len - final_len} duplicate reviews\")\n",
    "\n",
    "        print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_labels(df, min_label_count=20):\n",
    "    label_columns = ['atmosphere', 'narrative_structure', 'themes']\n",
    "    all_labels, label_counter = [], Counter()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_labels = []\n",
    "        for col in label_columns:\n",
    "            if col in df.columns:\n",
    "                value = row[col]\n",
    "                if value is not None and not (isinstance(value, float) and np.isnan(value)):\n",
    "                    if isinstance(value, str): labels = [l.strip().lower() for l in value.split(',') if l.strip()]\n",
    "                    elif isinstance(value, (list, np.ndarray)): labels = [str(i).strip().lower() for i in value if str(i).strip()]\n",
    "                    else: labels = [str(value).strip().lower()] if str(value).strip().lower() != 'nan' else []\n",
    "                    row_labels.extend(labels)\n",
    "                    label_counter.update(labels)\n",
    "        all_labels.append(row_labels)\n",
    "\n",
    "    frequent_labels = {label for label, count in label_counter.items() if count >= min_label_count}\n",
    "    filtered_labels = [[label for label in row_labels if label in frequent_labels] for row_labels in all_labels]\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    label_matrix = mlb.fit_transform(filtered_labels)\n",
    "\n",
    "    return label_matrix, mlb, label_counter\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    pos_counts = np.sum(y_train, axis=0)\n",
    "    neg_counts = len(y_train) - pos_counts\n",
    "    pos_weights = np.clip(neg_counts / (pos_counts + 1e-8), 1.0, 10.0)\n",
    "    return torch.FloatTensor(pos_weights).to(device)\n",
    "\n",
    "def compute_metrics_for_trainer(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.tensor(predictions)) > 0.5\n",
    "    predictions = predictions.numpy().astype(int)\n",
    "    labels = labels.astype(int)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n",
    "    return {'f1_macro': f1_macro}\n",
    "\n",
    "def get_validation_predictions(model, val_dataset):\n",
    "    print(\"\\nGetting predictions from the best model for the validation set...\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=32)\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_logits.append(outputs.logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    probabilities = torch.sigmoid(torch.cat(all_logits, dim=0))\n",
    "    true_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    return probabilities.numpy(), true_labels.numpy()\n",
    "\n",
    "def tune_thresholds(probabilities, true_labels, label_names):\n",
    "    print(\"\\nFinding optimal thresholds for each label...\")\n",
    "    best_thresholds = {}\n",
    "    for i in range(probabilities.shape[1]):\n",
    "        label_name = label_names[i]\n",
    "        y_prob = probabilities[:, i]\n",
    "        y_true = true_labels[:, i]\n",
    "\n",
    "        best_f1, best_thresh = 0, 0.5\n",
    "        for thresh in np.arange(0.1, 0.9, 0.01):\n",
    "            y_pred = (y_prob > thresh).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thresh = thresh\n",
    "\n",
    "        best_thresholds[label_name] = best_thresh\n",
    "        if i % 5 == 0:  # Print progress\n",
    "             print(f\"Processed label {i+1}/{len(label_names)}: '{label_name}' -> best thresh={best_thresh:.2f}\")\n",
    "    return best_thresholds\n",
    "\n",
    "def evaluate_with_optimal_thresholds(probabilities, true_labels, optimal_thresholds_dict, label_names):\n",
    "    print(\"\\nFinal Evaluation Report (with Optimal Thresholds)\")\n",
    "    thresholds_array = np.array([optimal_thresholds_dict[label] for label in label_names])\n",
    "    y_pred = (probabilities > thresholds_array).astype(int)\n",
    "\n",
    "    report = classification_report(true_labels, y_pred, target_names=label_names, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    f1_macro_optimal = f1_score(true_labels, y_pred, average='macro', zero_division=0)\n",
    "    return {'f1_macro_optimal': f1_macro_optimal}\n",
    "\n",
    "def run_pipeline():\n",
    "    # Part 1: Data Preparation\n",
    "    print(\"Part 1: Loading and Preparing Data\")\n",
    "    processing_df = pd.read_pickle(TRAINING_DATA_CLEANED_PATH) # If done in 2 parts with 2 training data files, use load and merge\n",
    "    label_matrix, mlb, label_counter = prepare_labels(processing_df, min_label_count=20)\n",
    "\n",
    "    X = processing_df['review'].values\n",
    "    y = label_matrix\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "    pos_weights = compute_class_weights(y_train)\n",
    "\n",
    "    # Part 2: Model and Trainer Setup\n",
    "    print(\"\\nPart 2: Setting up Model and Trainer\")\n",
    "    model_name = ZSC_MODEL\n",
    "    output_dir = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(mlb.classes_),\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "\n",
    "    train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "    loss_fn = WeightedBCELoss(pos_weights)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=6,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.2,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=150,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=150,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        learning_rate=1e-5,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        loss_fn=loss_fn,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Part 3: Training\n",
    "    print(\"\\nPart 3: Starting Model Training\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"Training completed successfully!\")\n",
    "        original_final_metrics = trainer.evaluate() # Get the final metrics with the 0.5 threshold\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Part 4: Post-Training Analysis\n",
    "    print(\"\\nPart 4: Post-Training Analysis\")\n",
    "\n",
    "    best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "    if not best_checkpoint_path:\n",
    "        print(\"Could not determine the best model checkpoint. Using the last model state.\")\n",
    "        best_model = trainer.model\n",
    "    else:\n",
    "        print(f\"Loading best model from checkpoint: {best_checkpoint_path}\")\n",
    "        best_model = AutoModelForSequenceClassification.from_pretrained(best_checkpoint_path)\n",
    "\n",
    "    probabilities, true_labels = get_validation_predictions(best_model, val_dataset)\n",
    "\n",
    "    # Find the optimal threshold for each label\n",
    "    optimal_thresholds = tune_thresholds(probabilities, true_labels, mlb.classes_)\n",
    "\n",
    "    # Save everything needed for future predictions\n",
    "    print(\"\\nSaving model, tokenizer, and all analysis artifacts...\")\n",
    "    trainer.save_model(output_dir) # Saves best model and tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model_info = {\n",
    "        'label_binarizer': mlb,\n",
    "        'label_counter': label_counter,\n",
    "        'optimal_thresholds': optimal_thresholds,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "    joblib.dump(model_info, os.path.join(output_dir, 'model_info.pkl'))\n",
    "    print(f\"All artifacts saved to: {output_dir}\")\n",
    "\n",
    "    new_final_metrics = evaluate_with_optimal_thresholds(probabilities, true_labels, optimal_thresholds, mlb.classes_)\n",
    "\n",
    "    print(\"\\n\\n================= FINAL PERFORMANCE SUMMARY =================\")\n",
    "    print(f\"Original F1 Macro: {original_final_metrics['eval_f1_macro']:.4f}\")\n",
    "    print(f\"Optimal F1 Macro (tuned thresholds):  {new_final_metrics['f1_macro_optimal']:.4f}\")\n",
    "    print(\"===========================================================\")\n",
    "\n",
    "def predict_with_optimal_thresholds(review_text, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model path not found: {model_path}\")\n",
    "\n",
    "    # Load all artifacts\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model_info = joblib.load(os.path.join(model_path, 'model_info.pkl'))\n",
    "\n",
    "    mlb = model_info['label_binarizer']\n",
    "    optimal_thresholds = model_info['optimal_thresholds']\n",
    "\n",
    "    # Ensure thresholds are in the correct order\n",
    "    thresholds_tensor = torch.tensor([optimal_thresholds[label] for label in mlb.classes_])\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    thresholds_tensor = thresholds_tensor.to(device)\n",
    "\n",
    "    # Tokenize and predict\n",
    "    encoding = tokenizer(review_text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.sigmoid(outputs.logits)\n",
    "        predictions = (probabilities > thresholds_tensor).cpu().numpy().astype(int)\n",
    "\n",
    "    predicted_labels = mlb.inverse_transform(predictions)\n",
    "\n",
    "    return predicted_labels[0] if predicted_labels else []\n",
    "\n",
    "def tune_thresholds_and_compare():\n",
    "    # Step 1: Load all necessary artifacts\n",
    "    print(\"Step 1: Loading model and data artifacts\")\n",
    "    if not os.path.exists(BEST_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"CRITICAL: The specified best model path does not exist: {BEST_MODEL_PATH}\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BEST_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_PATH)\n",
    "\n",
    "    model_info = joblib.load(MODEL_INFO_PATH)\n",
    "    mlb = model_info['label_binarizer']\n",
    "\n",
    "    # Step 2: Recreate the exact same validation set\n",
    "    print(\"\\nStep 2: Recreating the validation dataset\")\n",
    "\n",
    "    df_full = pd.read_pickle(TRAINING_DATA_CLEANED_PATH)\n",
    "\n",
    "    # Use the loaded binarizer to transform the labels\n",
    "    all_labels = []\n",
    "    label_columns = ['atmosphere', 'narrative_structure', 'themes']\n",
    "    for _, row in df_full.iterrows():\n",
    "        row_labels = []\n",
    "        for col in label_columns:\n",
    "            if col in df_full.columns:\n",
    "                value = row[col]\n",
    "                if value is not None and not (isinstance(value, float) and np.isnan(value)):\n",
    "                    if isinstance(value, str): labels = [l.strip().lower() for l in value.split(',') if l.strip()]\n",
    "                    elif isinstance(value, (list, np.ndarray)): labels = [str(i).strip().lower() for i in value if str(i).strip()]\n",
    "                    else: labels = [str(value).strip().lower()] if str(value).strip().lower() != 'nan' else []\n",
    "                    row_labels.extend(labels)\n",
    "        all_labels.append(row_labels)\n",
    "\n",
    "    y = mlb.transform(all_labels)\n",
    "    X = df_full['review'].values\n",
    "\n",
    "    # Use the same random_state to get the exact same split\n",
    "    _, X_val, _, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
    "    print(f\"Successfully recreated validation set with {len(val_dataset)} samples.\")\n",
    "\n",
    "    # Step 3: Run the analysis pipeline\n",
    "    print(\"\\nStep 3: Running the analysis pipeline\")\n",
    "    probabilities, true_labels = get_validation_predictions(model, val_dataset)\n",
    "    optimal_thresholds = tune_thresholds(probabilities, true_labels, mlb.classes_)\n",
    "    final_metrics = evaluate_with_optimal_thresholds(probabilities, true_labels, optimal_thresholds, mlb.classes_)\n",
    "\n",
    "    # Step 4: Display final results\n",
    "    print(\"\\n\\n================= FINAL PERFORMANCE SUMMARY =================\")\n",
    "    print(f\"Optimal F1 Macro (tuned thresholds):                  {final_metrics['f1_macro_optimal']:.4f}\")\n",
    "    print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline to get the full model saved with all model info\n",
    "\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# **WHEN RAN PIPELINE WITHOUT SAVING OPTIMAL_THRESHOLDS IN MODEL INFO**\n",
    "\n",
    "# You need to have the 'optimal_thresholds' dictionary available in your session.\n",
    "# If you don't, you must re-run the analysis part of the standalone script to get it.\n",
    "\n",
    "print(f\"Loading existing model info from: {MODEL_INFO_PATH}\")\n",
    "model_info = joblib.load(MODEL_INFO_PATH)\n",
    "\n",
    "print(\"Adding optimal thresholds to model_info...\")\n",
    "model_info['optimal_thresholds'] = optimal_thresholds # Add the thresholds dictionary\n",
    "\n",
    "# Overwrite the old file with the new, improved one\n",
    "joblib.dump(model_info, MODEL_INFO_PATH)\n",
    "\n",
    "print(\"\\nSUCCESS: Your optimal thresholds have been permanently saved!\")\n",
    "print(\"The file 'model_info.pkl' now contains your label binarizer AND your thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run\n",
    "my_review = \"The movie was a bit strange and surreal. It didn't have much of a plot, but the visuals were stunning and it felt like a dream.\"\n",
    "\n",
    "# Get the optimized predictions\n",
    "final_labels = predict_with_optimal_thresholds(review_text=my_review, model_path=BEST_MODEL_PATH)\n",
    "\n",
    "print(f\"Review: '{my_review}'\")\n",
    "print(f\"Predicted Labels: {final_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, SequentialSampler # Good for batching\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "FINAL_MODEL_DIR = BEST_MODEL_PATH\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def classify_reviews_batch_custom(reviews, model_path, batch_size = 32):\n",
    "    print(f\"Using Custom Fine-Tuned Model for Batch Classification\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model directory not found: {model_path}\")\n",
    "\n",
    "    # Load all artifacts ONCE\n",
    "    print(\"Loading model and artifacts...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model_info = joblib.load(os.path.join(model_path, 'model_info.pkl'))\n",
    "    mlb = model_info['label_binarizer']\n",
    "    optimal_thresholds = model_info['optimal_thresholds']\n",
    "\n",
    "    # Prepare model and thresholds for inference\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    thresholds_tensor = torch.tensor([optimal_thresholds[label] for label in mlb.classes_]).to(device)\n",
    "\n",
    "    # Process reviews in mini-batches\n",
    "    all_predicted_labels = []\n",
    "    print(f\"Starting classification of {len(reviews)} reviews with batch size {batch_size}...\")\n",
    "\n",
    "    for i in range(0, len(reviews), batch_size):\n",
    "        batch_reviews = reviews[i:i+batch_size]\n",
    "        print(f\"  Processing batch {i//batch_size + 1}...\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Tokenize the entire batch at once. `padding=True` handles different lengths.\n",
    "            encoding = tokenizer(\n",
    "                batch_reviews,\n",
    "                truncation=True,\n",
    "                padding=True, # Use dynamic padding for batches\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # Send the whole batch to the GPU\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            # Get model outputs for the batch\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = torch.sigmoid(outputs.logits)\n",
    "\n",
    "            # Apply thresholds to the entire batch\n",
    "            predictions = (probabilities > thresholds_tensor).cpu().numpy().astype(int)\n",
    "\n",
    "            # Decode the batch of predictions back into label names\n",
    "            batch_labels = mlb.inverse_transform(predictions)\n",
    "            all_predicted_labels.extend(batch_labels)\n",
    "\n",
    "    print(\"Batch classification complete.\")\n",
    "    return all_predicted_labels\n",
    "\n",
    "def separate_labels_by_category(all_labels):\n",
    "    atmosphere_results = []\n",
    "    theme_results = []\n",
    "    narrative_results = []\n",
    "\n",
    "    for labels in all_labels:\n",
    "        label_set = set(labels) if labels else set()\n",
    "\n",
    "        atmosphere = [label for label in labels if label in atmosphere_labels] if labels else []\n",
    "        themes = [label for label in labels if label in theme_candidates] if labels else []\n",
    "        narrative = [label for label in labels if label in narrative_labels] if labels else []\n",
    "\n",
    "        atmosphere_results.append(atmosphere)\n",
    "        theme_results.append(themes)\n",
    "        narrative_results.append(narrative)\n",
    "\n",
    "    return atmosphere_results, theme_results, narrative_results\n",
    "\n",
    "def create_classified_embeddings(df, model_path: str, batch_size: int = 32):\n",
    "    df['review'] = df['review'].fillna('')\n",
    "\n",
    "    # Load SentenceTransformer for embeddings\n",
    "    sbert_model = SentenceTransformer(SBERT_MODEL)\n",
    "\n",
    "    print(\"Starting classification with custom model...\")\n",
    "\n",
    "    # Filter out empty reviews for classification\n",
    "    valid_reviews = [(i, review) for i, review in enumerate(df['review'].tolist()) if review.strip()]\n",
    "\n",
    "    if not valid_reviews:\n",
    "        print(\"No valid reviews found for classification.\")\n",
    "        # Create empty results\n",
    "        all_labels = [[] for _ in range(len(df))]\n",
    "    else:\n",
    "        # Extract just the review texts for classification\n",
    "        review_texts = [review for _, review in valid_reviews]\n",
    "\n",
    "        # Classify using the custom model\n",
    "        classified_labels = classify_reviews_batch_custom(review_texts, model_path, batch_size)\n",
    "\n",
    "        # Map results back to original indices\n",
    "        results = {}\n",
    "        for j, (orig_idx, _) in enumerate(valid_reviews):\n",
    "            results[orig_idx] = list(classified_labels[j]) if classified_labels[j] else []\n",
    "\n",
    "        # Create final list with all indices\n",
    "        all_labels = [results.get(i, []) for i in range(len(df))]\n",
    "\n",
    "    print(f\"Completed classification. Processing {len([labels for labels in all_labels if labels])} non-empty classifications.\")\n",
    "\n",
    "    # Separate labels by category\n",
    "    print(\"Separating labels by category...\")\n",
    "    atmosphere_labels, theme_labels, narrative_labels = separate_labels_by_category(all_labels)\n",
    "\n",
    "    def encode_labels(labels):\n",
    "        \"\"\"Encode a list of labels into a single embedding vector.\"\"\"\n",
    "        if not labels:\n",
    "            return np.zeros(384, dtype='float32')\n",
    "        label_embeds = sbert_model.encode(labels, batch_size=32, show_progress_bar=False)\n",
    "        return np.mean(label_embeds, axis=0).astype('float32')\n",
    "\n",
    "    print(\"Encoding atmosphere labels...\")\n",
    "    atmosphere_emb = np.array([encode_labels(labels) for labels in atmosphere_labels])\n",
    "\n",
    "    print(\"Encoding narrative_structure labels...\")\n",
    "    narrative_emb = np.array([encode_labels(labels) for labels in narrative_labels])\n",
    "\n",
    "    print(\"Encoding themes labels...\")\n",
    "    themes_emb = np.array([encode_labels(labels) for labels in theme_labels])\n",
    "\n",
    "    print(\"Encoding combined labels...\")\n",
    "    combined_emb = np.array([encode_labels(labels) for labels in all_labels])\n",
    "\n",
    "    print(\"Completed label encoding.\")\n",
    "\n",
    "    # Create final DataFrame\n",
    "    final_df = pd.DataFrame({\n",
    "        'id': df['id'],\n",
    "        'atmosphere': atmosphere_labels,\n",
    "        'narrative_structure': narrative_labels,\n",
    "        'themes': theme_labels,\n",
    "        'atmosphere_emb': list(atmosphere_emb),\n",
    "        'narrative_emb': list(narrative_emb),\n",
    "        'themes_emb': list(themes_emb),\n",
    "        'combined_emb': list(combined_emb)\n",
    "    })\n",
    "\n",
    "    print(\"Sample results:\")\n",
    "    print(final_df.head())\n",
    "\n",
    "    non_empty_indices = [i for i, labels in enumerate(all_labels) if labels]\n",
    "    if non_empty_indices:\n",
    "        print(f\"\\nExample classifications:\")\n",
    "        for i, idx in enumerate(non_empty_indices[:5]):\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"Atmosphere: {atmosphere_labels[idx]}\")\n",
    "            print(f\"Themes: {theme_labels[idx]}\")\n",
    "            print(f\"Narrative: {narrative_labels[idx]}\")\n",
    "            print(f\"All labels: {all_labels[idx]}\")\n",
    "\n",
    "    output_path = CLASSIFIED_EMBEDDINGS_PATH\n",
    "    final_df.to_pickle(output_path)\n",
    "    print(f\"\\nSaved results to: {output_path}\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def test_create_classified_embeddings_custom(df):\n",
    "    test_df = df[df['title'] == 'Lost Highway'].copy()\n",
    "    print(test_df)\n",
    "    if test_df.empty:\n",
    "        raise ValueError(\"Movie Lost Highway not found in the dataframe\")\n",
    "\n",
    "    print(f\"Testing with movie ID: {test_df['id'].iloc[0]}\")\n",
    "    test_df['review'] = test_df['review'].fillna('')\n",
    "    reviews_to_classify = test_df['review'].tolist()\n",
    "\n",
    "    # Single classification call\n",
    "    all_labels = classify_reviews_batch_custom(reviews_to_classify, FINAL_MODEL_DIR)\n",
    "\n",
    "    print(\"\\nFiltering results into categories...\")\n",
    "    test_df['atmosphere'] = [[label for label in review_labels if label in atmosphere_labels] for review_labels in all_labels]\n",
    "    test_df['narrative_structure'] = [[label for label in review_labels if label in narrative_labels] for review_labels in all_labels]\n",
    "    test_df['themes'] = [[label for label in review_labels if label in theme_candidates] for review_labels in all_labels]\n",
    "\n",
    "    print(f\"Atmosphere results: {test_df['atmosphere'].iloc[0]}\")\n",
    "    print(f\"Narrative structure results: {test_df['narrative_structure'].iloc[0]}\")\n",
    "    print(f\"Themes results: {test_df['themes'].iloc[0]}\")\n",
    "\n",
    "    # Embedding\n",
    "    print(\"\\nStarting label encoding...\")\n",
    "    sbert_model = SentenceTransformer(SBERT_MODEL)\n",
    "\n",
    "    def encode_labels(labels):\n",
    "        if not labels:\n",
    "            return np.zeros(384, dtype='float32')\n",
    "        label_embeds = sbert_model.encode(labels, batch_size=32, show_progress_bar=False)\n",
    "        return np.mean(label_embeds, axis=0).astype('float32')\n",
    "\n",
    "    atmosphere_emb = np.array([encode_labels(labels) for labels in test_df['atmosphere']])\n",
    "    narrative_emb = np.array([encode_labels(labels) for labels in test_df['narrative_structure']])\n",
    "    themes_emb = np.array([encode_labels(labels) for labels in test_df['themes']])\n",
    "\n",
    "    print(\"Completed label encoding.\")\n",
    "\n",
    "    final_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'atmosphere_emb': list(atmosphere_emb),\n",
    "        'narrative_emb': list(narrative_emb),\n",
    "        'themes_emb': list(themes_emb),\n",
    "    })\n",
    "\n",
    "    print(\"\\nFinal test results with custom model:\")\n",
    "    print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.read_pickle(INITIAL_DATA_PATH)\n",
    "\n",
    "print(initial_df.head())\n",
    "\n",
    "final_df = create_classified_embeddings(initial_df, BEST_MODEL_PATH)\n",
    "\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = final_df[final_df['id'] == 13].copy()\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **IF FORGOT TO SAVE TOKENIZER**\n",
    "\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "FINAL_SAVE_PATH = BEST_MODEL_PATH\n",
    "\n",
    "BEST_CHECKPOINT_PATH = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved/checkpoint-600'\n",
    "\n",
    "MODEL_NAME = ZSC_MODEL\n",
    "\n",
    "print(f\"Loading components\")\n",
    "\n",
    "# Check if the checkpoint path exists\n",
    "if not os.path.exists(BEST_CHECKPOINT_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"CRITICAL: The checkpoint path was not found: {BEST_CHECKPOINT_PATH}\\n\"\n",
    "        \"Please check the folder and update the path to the correct checkpoint directory.\"\n",
    "    )\n",
    "\n",
    "# Load the best model from the specified checkpoint\n",
    "print(f\"Loading model from checkpoint: {BEST_CHECKPOINT_PATH}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BEST_CHECKPOINT_PATH)\n",
    "\n",
    "# CRITICAL STEP: Load a fresh tokenizer from the original Hugging Face source.\n",
    "# We do this because the tokenizer files are what's missing in your saved folder.\n",
    "print(f\"Loading fresh tokenizer from source: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"\\nSaving all components to the final directory\")\n",
    "\n",
    "# Ensure the final destination directory exists\n",
    "os.makedirs(FINAL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Save the model and the tokenizer to the FINAL, clean directory.\n",
    "# This will create all the necessary files, including vocab.json.\n",
    "model.save_pretrained(FINAL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(FINAL_SAVE_PATH)\n",
    "\n",
    "print(f\"\\nSuccessfully saved model and tokenizer to: {FINAL_SAVE_PATH}\")\n",
    "\n",
    "print(\"\\nVerifying the contents of the final directory ---\")\n",
    "# Let's list the files to make sure everything is there now.\n",
    "!ls -l \"{FINAL_SAVE_PATH}\"\n",
    "\n",
    "print(\"\\nVerification complete. You should see 'vocab.json' and 'merges.txt' in the list above.\")\n",
    "print(\"You can now safely run your 'predict_with_optimal_thresholds' or other functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
