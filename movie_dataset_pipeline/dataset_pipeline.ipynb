{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZXd3Iz1tqjgs",
        "outputId": "f4dfb433-94f5-432e-cded-6f38e33c166a"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu-cu12\n",
        "!pip install -U langchain-community\n",
        "!pip install langchain_openai\n",
        "!pip install langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HZyNVvHotFp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer, normalize\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import joblib\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq5OSuEqHwSZ",
        "outputId": "a80c5348-c099-4022-cdcd-5b6c188e3f02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "\n",
        "# Download NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBcs_X9BpFji"
      },
      "outputs": [],
      "source": [
        "GENRE_WEIGHT = 1.0\n",
        "KEYWORD_WEIGHT = 2.0\n",
        "VOTE_WEIGHT = 0.3\n",
        "ADULT_WEIGHT = 0.2\n",
        "SUMMARY_WEIGHT = 0.5\n",
        "ATMOSPHERE_WEIGHT = 0.5\n",
        "NARRATIVE_WEIGHT = 0.5\n",
        "EMOTION_WEIGHT = 0.5\n",
        "THEME_WEIGHT = 0.5\n",
        "CHARACTER_WEIGHT = 0.5\n",
        "PACING_WEIGHT = 0.5\n",
        "\n",
        "INITIAL_DATA_PATH = '/content/drive/My Drive/colab.pkl'\n",
        "BEST_MODEL_PATH = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved'\n",
        "MODEL_INFO_PATH = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved/model_info.pkl'\n",
        "NON_CLASSIFIED_EMBEDDINGS_PATH = '/content/drive/My Drive/movie_similarity/non_classified_embeddings.pkl'\n",
        "CLASSIFIED_EMBEDDINGS_PATH = '/content/drive/My Drive/movie_similarity/classified_embeddings.pkl'\n",
        "TRAINING_DATA_PATH = '/content/drive/My Drive/movie_similarity/training_data.pkl'\n",
        "TRAINING_DATA_CLEANED_PATH = '/content/drive/My Drive/movie_similarity/training_data_cleaned.pkl'\n",
        "SBERT_MODEL = 'all-MiniLM-L6-v2'\n",
        "ZSC_MODEL = 'FacebookAI/roberta-base'\n",
        "\n",
        "atmosphere_labels = [\n",
        "    'dark', 'tense', 'neutral',\n",
        "    'melancholic', 'cheerful', 'mysterious', 'foreboding',\n",
        "    'dreamy', 'chaotic', 'nostalgic', 'surreal',\n",
        "]\n",
        "\n",
        "narrative_labels = ['linear', 'non-linear', 'episodic', 'circular']\n",
        "\n",
        "theme_candidates = [\n",
        "    'redemption', 'betrayal', 'love', 'identity', 'family',\n",
        "    'loss', 'justice', 'freedom', 'corruption', 'hope',\n",
        "    'loyalty', 'power', 'isolation', 'transformation'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpMdXOwXpLCz"
      },
      "outputs": [],
      "source": [
        "def create_non_classified_embeddings(df):\n",
        "    df['genres'] = df['genres'].apply(lambda x: [item.strip() for item in x.split(',')] if x else [])\n",
        "    df['keywords'] = df['keywords'].apply(lambda x: [item.strip() for item in x.split(',')] if x else [])\n",
        "    df['overview'] = df['overview'].fillna('')\n",
        "\n",
        "    df['vote_average'] = pd.to_numeric(df['vote_average'], errors='coerce')\n",
        "    df['vote_average'] = df['vote_average'].fillna(df['vote_average'].mean())\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    vote_average_scaled = scaler.fit_transform(df[['vote_average']]).flatten()\n",
        "\n",
        "    model = SentenceTransformer(SBERT_MODEL)\n",
        "\n",
        "    genre_texts = [' '.join(genres) for genres in df['genres']]\n",
        "    keyword_texts = [' '.join(keywords) for keywords in df['keywords']]\n",
        "\n",
        "    all_texts = genre_texts + keyword_texts + df['overview'].tolist()\n",
        "    all_embeddings = model.encode(all_texts, show_progress_bar=True).astype('float32')\n",
        "\n",
        "    n_genres = len(genre_texts)\n",
        "    n_keywords = len(keyword_texts)\n",
        "    genres_emb = all_embeddings[:n_genres]\n",
        "    keywords_emb = all_embeddings[n_genres:n_genres+n_keywords]\n",
        "    summary_embeddings = all_embeddings[n_genres+n_keywords:]\n",
        "\n",
        "    final_df = pd.DataFrame({\n",
        "        'id': df['id'],\n",
        "        'overview_emb': list(summary_embeddings),\n",
        "        'genres_emb': list(genres_emb),\n",
        "        'keywords_emb': list(keywords_emb),\n",
        "        'vote_average_scaled': vote_average_scaled\n",
        "    })\n",
        "\n",
        "    print(final_df.head())\n",
        "\n",
        "    final_df.to_pickle(NON_CLASSIFIED_EMBEDDINGS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864,
          "referenced_widgets": [
            "9cdc69cb5974454a8b9c5e1d49e27c3b",
            "ec299e94d0b44b5ba5bdf0a4eecef4d9",
            "5a7ed2ab0bbf4c6685b334e9a69f2fa5",
            "e1a174fa1a154affbecf1ad1d5a7f942",
            "5e4df461efa244889d4bb048c7f93272",
            "2548e622e55645f999d6ad74b76d064e",
            "2bce33c12b0848aeb20b74be5653b5f2",
            "d620b7e619d44237a61c8f787f60308b",
            "6f4d368abf5547dda3d70f4feee3a456",
            "7f09a5880a174152844d0ea15daa6d25",
            "2879d2cb39b1495db465607c06d10991"
          ]
        },
        "id": "chP1FZW6cKF5",
        "outputId": "655a1b74-9b16-49dc-a8e5-ad0ce5e859f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id                title                                           overview  \\\n",
            "0   2                Ariel  A Finnish man goes to the city to find a job a...   \n",
            "1   3  Shadows in Paradise  Nikander, a rubbish collector and would-be ent...   \n",
            "2   5           Four Rooms  It's Ted the Bellhop's first night on the job....   \n",
            "3   6       Judgment Night  Four young friends, while taking a shortcut en...   \n",
            "4  11            Star Wars  Princess Leia is captured and held hostage by ...   \n",
            "\n",
            "   vote_average                            genres  \\\n",
            "0         7.100        Comedy,Crime,Drama,Romance   \n",
            "1         7.291              Comedy,Drama,Romance   \n",
            "2         5.864                            Comedy   \n",
            "3         6.455             Action,Crime,Thriller   \n",
            "4         8.206  Action,Adventure,Science Fiction   \n",
            "\n",
            "                                            keywords  \\\n",
            "0  factory worker,falling in love,helsinki, finla...   \n",
            "1               garbage,helsinki, finland,salesclerk   \n",
            "2  anthology,bet,hoodlum,hotel,hotel room,los ang...   \n",
            "3  boxing,chicago, illinois,drug dealer,escape,on...   \n",
            "4  android,awestruck,desert,empire,excited,freedo...   \n",
            "\n",
            "                                              review  \n",
            "0  Synopsis: A good looking young guy inherits an...  \n",
            "1  A simpleminded garbage man and a misanthropic ...  \n",
            "2  Four different rooms, four different episodes,...  \n",
            "3  I've heard this film dissed by so many, and I ...  \n",
            "4  I can never pick a favorite movie because diff...  \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cdc69cb5974454a8b9c5e1d49e27c3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/938 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id                                       overview_emb  \\\n",
            "0   2  [0.012737188, 0.013031937, -0.07787705, -0.039...   \n",
            "1   3  [-0.120130956, -0.0150178075, -0.01258817, -0....   \n",
            "2   5  [-0.073654644, -0.008337193, 0.022501769, 0.01...   \n",
            "3   6  [0.093476705, 0.08604528, -0.049391024, -0.011...   \n",
            "4  11  [-0.061168995, -0.039084088, 0.055229034, -0.0...   \n",
            "\n",
            "                                          genres_emb  \\\n",
            "0  [-0.07958339, -0.06317735, -0.04624551, 0.0377...   \n",
            "1  [-0.056528702, -0.07941368, -0.029316708, 0.07...   \n",
            "2  [-0.042611323, -0.046201073, -0.03457101, 0.01...   \n",
            "3  [-0.042185202, -0.00575423, -0.08346808, 0.032...   \n",
            "4  [-0.024891889, -0.02195114, -0.0148923695, 0.0...   \n",
            "\n",
            "                                        keywords_emb  vote_average_scaled  \n",
            "0  [-0.067110725, 0.029397095, 0.0063949055, 0.04...             0.571505  \n",
            "1  [-0.038277473, 0.065131254, -0.037371606, -0.0...             0.622275  \n",
            "2  [0.04833135, -0.05277274, -0.05252021, 0.02396...             0.242956  \n",
            "3  [0.0026469708, 0.0919033, -0.07634911, -0.0425...             0.400053  \n",
            "4  [-0.052738547, -0.004225087, -0.05216478, -0.1...             0.865497  \n"
          ]
        }
      ],
      "source": [
        "initial_df = pd.read_pickle(INITIAL_DATA_PATH)\n",
        "\n",
        "print(initial_df.head())\n",
        "\n",
        "create_non_classified_embeddings(initial_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBDzwQEhUQCA"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import datetime\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SequentialChain, SimpleSequentialChain\n",
        "from langchain.memory import ConversationBufferWindowMemory, FileChatMessageHistory, ConversationSummaryBufferMemory\n",
        "from langchain.agents import AgentType, initialize_agent, Tool\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    google_api_key=\"your-gemini-api-key\",\n",
        "    model=\"gemini-2.0-flash\", # would be better to use gemini 2.5 flash, this model sometimes generates labels that are not specified\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "def create_labeled_data(model, df):\n",
        "    system_prompt = PromptTemplate(\n",
        "        input_variables=[\"query\"],\n",
        "        template=\n",
        "        f\"\"\"\n",
        "        You are a highly analytical and precise movie review labeler. Your job is to determine the movie's features based *only* on the provided user review. You will classify the movie features into specific categories. You are given very specific instructions for this and must not deviate from the categories or their definitions.\n",
        "\n",
        "        The possible labels for atmosphere: {atmosphere_labels}. **YOU MUST CHOOSE ONLY FROM THIS LIST.**\n",
        "        The possible labels for narrative_structure: {narrative_labels}. **YOU MUST CHOOSE ONLY FROM THIS LIST.**\n",
        "        The possible labels for themes: {theme_candidates}. **YOU MUST CHOOSE ONLY FROM THIS LIST.**\n",
        "\n",
        "         **Instructions for Themes:**\n",
        "        - Only include a theme if it is strongly and unambiguously expressed or implied by the review.\n",
        "        - Be particularly careful with 'hope' and 'redemption'. These should only be applied if there is a clear, sustained sense of optimism or a definitive moral turnaround, respectively. If a film introduces hope only to crush it, or shows no clear path to absolution, do NOT label it with 'hope' or 'redemption'.\n",
        "\n",
        "        **CRITICAL RULE: ALL LABELS YOU PROVIDE MUST BE EXACTLY MATCHED FROM THE RESPECTIVE ALLOWED LISTS. DO NOT INVENT NEW LABELS. IF NO APPLICABLE LABEL FROM THE LISTS IS FOUND FOR A CATEGORY, PROVIDE AN EMPTY LIST [] FOR THAT CATEGORY.**\n",
        "\n",
        "        The output format of your answer should be: ATMOSPHERE: (list of labels you determined) ; NARRATIVE_STRUCTURE: (list of labels you determined) ; THEMES: (list of labels you determined)\n",
        "\n",
        "        Query: {{query}}\n",
        "        Answer:\"\"\"\n",
        "    )\n",
        "\n",
        "    batch_size = 4\n",
        "    max_retries = 3\n",
        "    retry_delay = 5\n",
        "\n",
        "    chain = LLMChain(llm=model, prompt=system_prompt)\n",
        "\n",
        "    processing_df = df.copy().reset_index(drop=True)\n",
        "\n",
        "    if processing_df.empty:\n",
        "        print(\"Warning: Input DataFrame is empty. Returning an empty DataFrame.\")\n",
        "        return pd.DataFrame(columns=['movie_id', 'review', 'atmosphere', 'narrative_structure', 'themes'])\n",
        "\n",
        "    print(f\"Processing {len(processing_df)} reviews in batches of {batch_size}...\")\n",
        "    print(processing_df.head())\n",
        "\n",
        "    processing_df['review'] = processing_df['review'].fillna('').astype(str)\n",
        "\n",
        "    processing_df['atmosphere'] = None\n",
        "    processing_df['narrative_structure'] = None\n",
        "    processing_df['themes'] = None\n",
        "\n",
        "    list_content_regex = re.compile(r\"\\[\\s*([^\\]]*?)\\s*\\]\")\n",
        "\n",
        "    def parse_labels_from_llm_output(text: str) -> Tuple[List[str], List[str], List[str]]:\n",
        "        atmosphere = []\n",
        "        narrative_structure = []\n",
        "        themes = []\n",
        "\n",
        "        try:\n",
        "            atmosphere_match = re.search(r'ATMOSPHERE:\\s*\\[(.*?)\\]', text)\n",
        "            if atmosphere_match:\n",
        "                atmosphere_str = atmosphere_match.group(1)\n",
        "                atmosphere = [label.strip().strip(\"'\\\"\") for label in atmosphere_str.split(',') if label.strip()]\n",
        "\n",
        "            narrative_match = re.search(r'NARRATIVE_STRUCTURE:\\s*\\[(.*?)\\]', text)\n",
        "            if narrative_match:\n",
        "                narrative_str = narrative_match.group(1)\n",
        "                narrative_structure = [label.strip().strip(\"'\\\"\") for label in narrative_str.split(',') if label.strip()]\n",
        "\n",
        "            themes_match = re.search(r'THEMES:\\s*\\[(.*?)\\]', text)\n",
        "            if themes_match:\n",
        "                themes_str = themes_match.group(1)\n",
        "                themes = [label.strip().strip(\"'\\\"\") for label in themes_str.split(',') if label.strip()]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing labels from LLM output: {e}\")\n",
        "            print(f\"Raw LLM output causing error: {text[:200]}...\")\n",
        "            return [], [], []\n",
        "\n",
        "        return atmosphere, narrative_structure, themes\n",
        "\n",
        "    total_reviews = len(processing_df)\n",
        "\n",
        "    for i in range(0, total_reviews, batch_size):\n",
        "        batch_start_time = time.time()\n",
        "        batch_end_index = min(i + batch_size, total_reviews)\n",
        "        current_batch_df = processing_df.iloc[i:batch_end_index]\n",
        "\n",
        "        inputs_for_current_batch = [{\"query\": review_text} for review_text in current_batch_df['review'].tolist()]\n",
        "\n",
        "        print(f\"\\nProcessing batch {i // batch_size + 1}/{(total_reviews + batch_size - 1) // batch_size} (Reviews {i}-{batch_end_index-1})\")\n",
        "\n",
        "        retries = 0\n",
        "        batch_labeled_results = []\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                batch_labeled_results = chain.batch(inputs_for_current_batch)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "                current_delay = retry_delay * (2 ** (retries - 1))\n",
        "                print(f\"Error during LLM batch call (Batch {i // batch_size + 1}): {e}\")\n",
        "                print(f\"Retrying in {current_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
        "                time.sleep(current_delay)\n",
        "\n",
        "        if not batch_labeled_results:\n",
        "            print(f\"Failed to get results for batch {i // batch_size + 1} after {max_retries} retries. Skipping this batch.\")\n",
        "            for j in range(len(current_batch_df)):\n",
        "                row_index = i + j\n",
        "                processing_df.at[row_index, 'atmosphere'] = []\n",
        "                processing_df.at[row_index, 'narrative_structure'] = []\n",
        "                processing_df.at[row_index, 'themes'] = []\n",
        "            continue\n",
        "\n",
        "        for j, result in enumerate(batch_labeled_results):\n",
        "            row_index = i + j\n",
        "\n",
        "            if isinstance(result, dict):\n",
        "                result_text = result.get('text', str(result))\n",
        "            else:\n",
        "                result_text = str(result)\n",
        "\n",
        "            atmosphere, narrative_structure, themes = parse_labels_from_llm_output(result_text)\n",
        "\n",
        "            processing_df.at[row_index, 'atmosphere'] = atmosphere\n",
        "            processing_df.at[row_index, 'narrative_structure'] = narrative_structure\n",
        "            processing_df.at[row_index, 'themes'] = themes\n",
        "\n",
        "        batch_elapsed_time = time.time() - batch_start_time\n",
        "        print(f\"Batch {i // batch_size + 1} completed in {batch_elapsed_time:.2f} seconds.\")\n",
        "        if batch_elapsed_time < 1.0:\n",
        "            time.sleep(1.0 - batch_elapsed_time)\n",
        "\n",
        "    print(f\"\\nLabeling complete. Successfully processed {len(processing_df)} reviews\")\n",
        "    print(processing_df[['id', 'review', 'atmosphere', 'narrative_structure', 'themes']].head())\n",
        "\n",
        "    try:\n",
        "        print(processing_df[['atmosphere', 'narrative_structure', 'themes']].apply(lambda col: col.explode().value_counts()).T.fillna(0).astype(int))\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying label counts: {e}\")\n",
        "        print(\"Label assignment completed successfully\")\n",
        "\n",
        "    processing_df.to_pickle(TRAINING_DATA_PATH)\n",
        "\n",
        "    return processing_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SafoKNXAWqyF",
        "outputId": "62282772-1f9f-4a46-9b1a-36574f930ea3"
      },
      "outputs": [],
      "source": [
        "initial_df = pd.read_pickle(INITIAL_DATA_PATH)\n",
        "\n",
        "print(initial_df.head())\n",
        "\n",
        "create_labeled_data(gemini, initial_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qujqONfcU_Tj",
        "outputId": "63233283-3442-40ad-8d5e-b98f37c01b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning atmosphere...\n",
            "  Before: 7997 non-null entries\n",
            "  After: 7725 non-null entries\n",
            "  Removed: 272 entries\n",
            "Cleaning narrative_structure...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9-1096829090.py:15: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if pd.isna(value):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Before: 7997 non-null entries\n",
            "  After: 7638 non-null entries\n",
            "  Removed: 359 entries\n",
            "Cleaning themes...\n",
            "  Before: 7997 non-null entries\n",
            "  After: 7539 non-null entries\n",
            "  Removed: 458 entries\n",
            "\n",
            "Removed 2099 rows with no valid labels\n",
            "\n",
            "Final DataFrame shape: (7898, 10)\n",
            "Original DataFrame shape: (9997, 10)\n",
            "\n",
            "Cleaned DataFrame saved as 'training_cleaned.pkl'\n",
            "\n",
            "Final unique labels:\n",
            "atmosphere: ['chaotic', 'cheerful', 'dark', 'dreamy', 'foreboding', 'melancholic', 'mysterious', 'neutral', 'nostalgic', 'surreal', 'tense']\n",
            "narrative_structure: ['circular', 'episodic', 'linear', 'non-linear']\n",
            "themes: ['betrayal', 'corruption', 'family', 'freedom', 'hope', 'identity', 'isolation', 'justice', 'loss', 'love', 'loyalty', 'power', 'redemption', 'transformation']\n"
          ]
        }
      ],
      "source": [
        "training_df = pd.read_pickle(TRAINING_DATA_PATH)\n",
        "\n",
        "expected_labels = {\n",
        "    'atmosphere': set(atmosphere_labels),\n",
        "    'narrative_structure': set(narrative_labels),\n",
        "    'themes': set(theme_candidates)\n",
        "}\n",
        "\n",
        "def clean_labels(value, valid_labels):\n",
        "    if value is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        labels = [label.strip() for label in value.split(',') if label.strip()]\n",
        "    elif isinstance(value, (list, np.ndarray)):\n",
        "        labels = [str(item).strip() for item in value if str(item).strip()]\n",
        "    else:\n",
        "        labels = [str(value).strip()]\n",
        "\n",
        "    clean_labels = [label for label in labels if label in valid_labels]\n",
        "\n",
        "    if clean_labels:\n",
        "        return ', '.join(clean_labels)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "clean_df = training_df.copy()\n",
        "\n",
        "for col in ['atmosphere', 'narrative_structure', 'themes']:\n",
        "    if col in clean_df.columns:\n",
        "        print(f\"Cleaning {col}...\")\n",
        "        before_count = clean_df[col].notna().sum()\n",
        "\n",
        "        clean_df[col] = clean_df[col].apply(lambda x: clean_labels(x, expected_labels[col]))\n",
        "\n",
        "        after_count = clean_df[col].notna().sum()\n",
        "        print(f\"Before: {before_count} non-null entries\")\n",
        "        print(f\"After: {after_count} non-null entries\")\n",
        "        print(f\"Removed: {before_count - after_count} entries\")\n",
        "\n",
        "label_columns = ['atmosphere', 'narrative_structure', 'themes']\n",
        "existing_label_cols = [col for col in label_columns if col in clean_df.columns]\n",
        "\n",
        "if existing_label_cols:\n",
        "    before_rows = len(clean_df)\n",
        "    clean_df = clean_df.dropna(subset=existing_label_cols, how='all')\n",
        "    after_rows = len(clean_df)\n",
        "    print(f\"\\nRemoved {before_rows - after_rows} rows with no valid labels\")\n",
        "\n",
        "print(f\"\\nFinal DataFrame shape: {clean_df.shape}\")\n",
        "print(f\"Original DataFrame shape: {initial_df.shape}\")\n",
        "\n",
        "clean_df.to_pickle(TRAINING_DATA_CLEANED_PATH)\n",
        "print(\"\\nCleaned DataFrame saved.\")\n",
        "\n",
        "print(\"\\nFinal unique labels:\")\n",
        "for col in existing_label_cols:\n",
        "    all_labels = set()\n",
        "    for value in clean_df[col].dropna():\n",
        "        if isinstance(value, str):\n",
        "            labels = [label.strip() for label in value.split(',') if label.strip()]\n",
        "            all_labels.update(labels)\n",
        "    print(f\"{col}: {sorted(list(all_labels))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liHt7R2KroWd",
        "outputId": "ac516858-880c-4056-a250-0bd0c0145ae5"
      },
      "outputs": [],
      "source": [
        "total_mislabels = 0\n",
        "\n",
        "for col in ['atmosphere', 'narrative_structure', 'themes']:\n",
        "    if col in clean_df.columns:\n",
        "        print(f\"\\n{col.upper()}\")\n",
        "        mislabel_count = 0\n",
        "        mislabel_examples = []\n",
        "\n",
        "        for idx, value in clean_df[col].dropna().items():\n",
        "            if isinstance(value, str):\n",
        "                labels = [label.strip() for label in value.split(',') if label.strip()]\n",
        "            elif isinstance(value, (list, np.ndarray)):\n",
        "                labels = [str(item).strip() for item in value if str(item).strip()]\n",
        "            else:\n",
        "                labels = [str(value).strip()]\n",
        "\n",
        "            bad_labels = [label for label in labels if label not in expected_labels[col]]\n",
        "            if bad_labels:\n",
        "                mislabel_count += 1\n",
        "                mislabel_examples.append((idx, bad_labels, labels))\n",
        "\n",
        "        print(f\"Bad entries: {mislabel_count}\")\n",
        "        print(f\"Bad labels found: {set([label for _, bad_labels, _ in mislabel_examples for label in bad_labels])}\")\n",
        "\n",
        "        # Show some examples\n",
        "        if mislabel_examples:\n",
        "            print(\"Examples of bad entries:\")\n",
        "            for idx, bad_labels, all_labels in mislabel_examples[:3]:\n",
        "                print(f\"Row {idx}: {all_labels} (bad: {bad_labels})\")\n",
        "\n",
        "        total_mislabels += total_mislabels\n",
        "\n",
        "print(f\"\\nTotal bad entries across all columns: {total_mislabels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYiMjPnqsSUJ",
        "outputId": "90ca3f98-5214-42a0-c9b3-c1d283cac545"
      },
      "outputs": [],
      "source": [
        "test_df = clean_df[clean_df['id'] == 13].copy()\n",
        "\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "H9ik2xXokC5o",
        "outputId": "a5bbe83e-bc22-4144-e130-bb3a19eb6f4f"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)        # Show all rows (for the 100 samples)\n",
        "pd.set_option('display.max_columns', None)     # Show all columns\n",
        "pd.set_option('display.max_colwidth', None)    # Show full content of each cell (important for reviews)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "random_samples_df = clean_df.sample(n=100, random_state=42)\n",
        "\n",
        "print(random_samples_df[['id', 'review', 'atmosphere', 'narrative_structure', 'themes']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLB9YaYuuvt",
        "outputId": "25d2423c-c13c-4415-ccd5-289e9d7f1661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
        "from sklearn.metrics import classification_report, hamming_loss, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "class WeightedBCELoss(nn.Module):\n",
        "    def __init__(self, pos_weights):\n",
        "        super(WeightedBCELoss, self).__init__()\n",
        "        self.pos_weights = pos_weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        return F.binary_cross_entropy_with_logits(\n",
        "            inputs, targets, pos_weight=self.pos_weights\n",
        "        )\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, tokenizer, max_length=256):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            review,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(labels)\n",
        "        }\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, loss_fn=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        if self.loss_fn is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def load_and_merge_data(path1, path2):\n",
        "    try:\n",
        "        df1 = pd.read_pickle(path1)\n",
        "        print(f\"Dataset 1 shape: {df1.shape}\")\n",
        "\n",
        "        try:\n",
        "            df2 = pd.read_pickle(path2)\n",
        "            print(f\"Dataset 2 shape: {df2.shape}\")\n",
        "            if set(df1.columns) != set(df2.columns):\n",
        "                common_cols = list(set(df1.columns) & set(df2.columns))\n",
        "                df1 = df1[common_cols]\n",
        "                df2 = df2[common_cols]\n",
        "            combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"Second dataset not found, using only the first dataset\")\n",
        "            combined_df = df1\n",
        "\n",
        "        if 'review' in combined_df.columns:\n",
        "            initial_len = len(combined_df)\n",
        "            combined_df = combined_df.drop_duplicates(subset=['review'], keep='first')\n",
        "            final_len = len(combined_df)\n",
        "            if initial_len != final_len:\n",
        "                print(f\"Removed {initial_len - final_len} duplicate reviews\")\n",
        "\n",
        "        print(f\"Combined dataset shape: {combined_df.shape}\")\n",
        "        return combined_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        raise\n",
        "\n",
        "def prepare_labels(df, min_label_count=20):\n",
        "    label_columns = ['atmosphere', 'narrative_structure', 'themes']\n",
        "    all_labels, label_counter = [], Counter()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_labels = []\n",
        "        for col in label_columns:\n",
        "            if col in df.columns:\n",
        "                value = row[col]\n",
        "                if value is not None and not (isinstance(value, float) and np.isnan(value)):\n",
        "                    if isinstance(value, str): labels = [l.strip().lower() for l in value.split(',') if l.strip()]\n",
        "                    elif isinstance(value, (list, np.ndarray)): labels = [str(i).strip().lower() for i in value if str(i).strip()]\n",
        "                    else: labels = [str(value).strip().lower()] if str(value).strip().lower() != 'nan' else []\n",
        "                    row_labels.extend(labels)\n",
        "                    label_counter.update(labels)\n",
        "        all_labels.append(row_labels)\n",
        "\n",
        "    frequent_labels = {label for label, count in label_counter.items() if count >= min_label_count}\n",
        "    filtered_labels = [[label for label in row_labels if label in frequent_labels] for row_labels in all_labels]\n",
        "\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    label_matrix = mlb.fit_transform(filtered_labels)\n",
        "\n",
        "    return label_matrix, mlb, label_counter\n",
        "\n",
        "def compute_class_weights(y_train):\n",
        "    pos_counts = np.sum(y_train, axis=0)\n",
        "    neg_counts = len(y_train) - pos_counts\n",
        "    pos_weights = np.clip(neg_counts / (pos_counts + 1e-8), 1.0, 10.0)\n",
        "    return torch.FloatTensor(pos_weights).to(device)\n",
        "\n",
        "def compute_metrics_for_trainer(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = torch.sigmoid(torch.tensor(predictions)) > 0.5\n",
        "    predictions = predictions.numpy().astype(int)\n",
        "    labels = labels.astype(int)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n",
        "    return {'f1_macro': f1_macro}\n",
        "\n",
        "def get_validation_predictions(model, val_dataset):\n",
        "    print(\"\\nGetting predictions from the best model for the validation set...\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=32)\n",
        "    all_logits, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            all_logits.append(outputs.logits.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    probabilities = torch.sigmoid(torch.cat(all_logits, dim=0))\n",
        "    true_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    return probabilities.numpy(), true_labels.numpy()\n",
        "\n",
        "def tune_thresholds(probabilities, true_labels, label_names):\n",
        "    print(\"\\nFinding optimal thresholds for each label...\")\n",
        "    best_thresholds = {}\n",
        "    for i in range(probabilities.shape[1]):\n",
        "        label_name = label_names[i]\n",
        "        y_prob = probabilities[:, i]\n",
        "        y_true = true_labels[:, i]\n",
        "\n",
        "        best_f1, best_thresh = 0, 0.5\n",
        "        for thresh in np.arange(0.1, 0.9, 0.01):\n",
        "            y_pred = (y_prob > thresh).astype(int)\n",
        "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thresh = thresh\n",
        "\n",
        "        best_thresholds[label_name] = best_thresh\n",
        "        if i % 5 == 0:  # Print progress\n",
        "             print(f\"Processed label {i+1}/{len(label_names)}: '{label_name}' -> best thresh={best_thresh:.2f}\")\n",
        "    return best_thresholds\n",
        "\n",
        "def evaluate_with_optimal_thresholds(probabilities, true_labels, optimal_thresholds_dict, label_names):\n",
        "    print(\"\\nFinal Evaluation Report (with Optimal Thresholds)\")\n",
        "    thresholds_array = np.array([optimal_thresholds_dict[label] for label in label_names])\n",
        "    y_pred = (probabilities > thresholds_array).astype(int)\n",
        "\n",
        "    report = classification_report(true_labels, y_pred, target_names=label_names, zero_division=0)\n",
        "    print(report)\n",
        "\n",
        "    f1_macro_optimal = f1_score(true_labels, y_pred, average='macro', zero_division=0)\n",
        "    return {'f1_macro_optimal': f1_macro_optimal}\n",
        "\n",
        "def run_pipeline():\n",
        "    # Part 1: Data Preparation\n",
        "    print(\"Part 1: Loading and Preparing Data\")\n",
        "    processing_df = pd.read_pickle(TRAINING_DATA_CLEANED_PATH) # If done in 2 parts with 2 training data files, use load and merge\n",
        "    label_matrix, mlb, label_counter = prepare_labels(processing_df, min_label_count=20)\n",
        "\n",
        "    X = processing_df['review'].values\n",
        "    y = label_matrix\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
        "\n",
        "    pos_weights = compute_class_weights(y_train)\n",
        "\n",
        "    # Part 2: Model and Trainer Setup\n",
        "    print(\"\\nPart 2: Setting up Model and Trainer\")\n",
        "    model_name = ZSC_MODEL\n",
        "    output_dir = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved'\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(mlb.classes_),\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "    train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
        "    val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
        "\n",
        "    loss_fn = WeightedBCELoss(pos_weights)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=6,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_ratio=0.2,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=150,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=150,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=150,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "        learning_rate=1e-5,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        loss_fn=loss_fn,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics_for_trainer,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "\n",
        "    # Part 3: Training\n",
        "    print(\"\\nPart 3: Starting Model Training\")\n",
        "    try:\n",
        "        train_result = trainer.train()\n",
        "        print(\"Training completed successfully!\")\n",
        "        original_final_metrics = trainer.evaluate() # Get the final metrics with the 0.5 threshold\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Part 4: Post-Training Analysis\n",
        "    print(\"\\nPart 4: Post-Training Analysis\")\n",
        "\n",
        "    best_checkpoint_path = trainer.state.best_model_checkpoint\n",
        "    if not best_checkpoint_path:\n",
        "        print(\"Could not determine the best model checkpoint. Using the last model state.\")\n",
        "        best_model = trainer.model\n",
        "    else:\n",
        "        print(f\"Loading best model from checkpoint: {best_checkpoint_path}\")\n",
        "        best_model = AutoModelForSequenceClassification.from_pretrained(best_checkpoint_path)\n",
        "\n",
        "    probabilities, true_labels = get_validation_predictions(best_model, val_dataset)\n",
        "\n",
        "    # Find the optimal threshold for each label\n",
        "    optimal_thresholds = tune_thresholds(probabilities, true_labels, mlb.classes_)\n",
        "\n",
        "    # Save everything needed for future predictions\n",
        "    print(\"\\nSaving model, tokenizer, and all analysis artifacts...\")\n",
        "    trainer.save_model(output_dir) # Saves best model and tokenizer\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    model_info = {\n",
        "        'label_binarizer': mlb,\n",
        "        'label_counter': label_counter,\n",
        "        'optimal_thresholds': optimal_thresholds,\n",
        "        'model_name': model_name\n",
        "    }\n",
        "    joblib.dump(model_info, os.path.join(output_dir, 'model_info.pkl'))\n",
        "    print(f\"All artifacts saved to: {output_dir}\")\n",
        "\n",
        "    new_final_metrics = evaluate_with_optimal_thresholds(probabilities, true_labels, optimal_thresholds, mlb.classes_)\n",
        "\n",
        "    print(\"\\n\\n================= FINAL PERFORMANCE SUMMARY =================\")\n",
        "    print(f\"Original F1 Macro: {original_final_metrics['eval_f1_macro']:.4f}\")\n",
        "    print(f\"Optimal F1 Macro (tuned thresholds):  {new_final_metrics['f1_macro_optimal']:.4f}\")\n",
        "    print(\"===========================================================\")\n",
        "\n",
        "def predict_with_optimal_thresholds(review_text, model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model path not found: {model_path}\")\n",
        "\n",
        "    # Load all artifacts\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model_info = joblib.load(os.path.join(model_path, 'model_info.pkl'))\n",
        "\n",
        "    mlb = model_info['label_binarizer']\n",
        "    optimal_thresholds = model_info['optimal_thresholds']\n",
        "\n",
        "    # Ensure thresholds are in the correct order\n",
        "    thresholds_tensor = torch.tensor([optimal_thresholds[label] for label in mlb.classes_])\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    thresholds_tensor = thresholds_tensor.to(device)\n",
        "\n",
        "    # Tokenize and predict\n",
        "    encoding = tokenizer(review_text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probabilities = torch.sigmoid(outputs.logits)\n",
        "        predictions = (probabilities > thresholds_tensor).cpu().numpy().astype(int)\n",
        "\n",
        "    predicted_labels = mlb.inverse_transform(predictions)\n",
        "\n",
        "    return predicted_labels[0] if predicted_labels else []\n",
        "\n",
        "def tune_thresholds_and_compare():\n",
        "    # Step 1: Load all necessary artifacts\n",
        "    print(\"Step 1: Loading model and data artifacts\")\n",
        "    if not os.path.exists(BEST_MODEL_PATH):\n",
        "        raise FileNotFoundError(f\"CRITICAL: The specified best model path does not exist: {BEST_MODEL_PATH}\")\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BEST_MODEL_PATH)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_PATH)\n",
        "\n",
        "    model_info = joblib.load(MODEL_INFO_PATH)\n",
        "    mlb = model_info['label_binarizer']\n",
        "\n",
        "    # Step 2: Recreate the exact same validation set\n",
        "    print(\"\\nStep 2: Recreating the validation dataset\")\n",
        "\n",
        "    df_full = pd.read_pickle(TRAINING_DATA_CLEANED_PATH)\n",
        "\n",
        "    # Use the loaded binarizer to transform the labels\n",
        "    all_labels = []\n",
        "    label_columns = ['atmosphere', 'narrative_structure', 'themes']\n",
        "    for _, row in df_full.iterrows():\n",
        "        row_labels = []\n",
        "        for col in label_columns:\n",
        "            if col in df_full.columns:\n",
        "                value = row[col]\n",
        "                if value is not None and not (isinstance(value, float) and np.isnan(value)):\n",
        "                    if isinstance(value, str): labels = [l.strip().lower() for l in value.split(',') if l.strip()]\n",
        "                    elif isinstance(value, (list, np.ndarray)): labels = [str(i).strip().lower() for i in value if str(i).strip()]\n",
        "                    else: labels = [str(value).strip().lower()] if str(value).strip().lower() != 'nan' else []\n",
        "                    row_labels.extend(labels)\n",
        "        all_labels.append(row_labels)\n",
        "\n",
        "    y = mlb.transform(all_labels)\n",
        "    X = df_full['review'].values\n",
        "\n",
        "    # Use the same random_state to get the exact same split\n",
        "    _, X_val, _, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
        "    print(f\"Successfully recreated validation set with {len(val_dataset)} samples.\")\n",
        "\n",
        "    # Step 3: Run the analysis pipeline\n",
        "    print(\"\\nStep 3: Running the analysis pipeline\")\n",
        "    probabilities, true_labels = get_validation_predictions(model, val_dataset)\n",
        "    optimal_thresholds = tune_thresholds(probabilities, true_labels, mlb.classes_)\n",
        "    final_metrics = evaluate_with_optimal_thresholds(probabilities, true_labels, optimal_thresholds, mlb.classes_)\n",
        "\n",
        "    # Step 4: Display final results\n",
        "    print(\"\\n\\n================= FINAL PERFORMANCE SUMMARY =================\")\n",
        "    print(f\"Optimal F1 Macro (tuned thresholds):                  {final_metrics['f1_macro_optimal']:.4f}\")\n",
        "    print(\"===========================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "r0Zjgi86Kskh",
        "outputId": "ac109521-1dd3-4e17-ffbc-a3c49986a9cb"
      },
      "outputs": [],
      "source": [
        "# Run pipeline to get the full model saved with all model info\n",
        "\n",
        "run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "VZ02inq4hLd5",
        "outputId": "77a63246-d4df-47dd-bd34-ed748a608a9f"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# **WHEN RAN PIPELINE WITHOUT SAVING OPTIMAL_THRESHOLDS IN MODEL INFO**\n",
        "\n",
        "# You need to have the 'optimal_thresholds' dictionary available in your session.\n",
        "# If you don't, you must re-run the analysis part of the standalone script to get it.\n",
        "\n",
        "print(f\"Loading existing model info from: {MODEL_INFO_PATH}\")\n",
        "model_info = joblib.load(MODEL_INFO_PATH)\n",
        "\n",
        "print(\"Adding optimal thresholds to model_info...\")\n",
        "model_info['optimal_thresholds'] = optimal_thresholds # Add the thresholds dictionary\n",
        "\n",
        "# Overwrite the old file with the new, improved one\n",
        "joblib.dump(model_info, MODEL_INFO_PATH)\n",
        "\n",
        "print(\"\\nSUCCESS: Your optimal thresholds have been permanently saved!\")\n",
        "print(\"The file 'model_info.pkl' now contains your label binarizer AND your thresholds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9z6F-L8SNsh",
        "outputId": "2f556c07-8145-4884-c4c0-5896d8ec460f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: 'The movie was a bit strange and surreal. It didn't have much of a plot, but the visuals were stunning and it felt like a dream.'\n",
            "Predicted Labels: ('chaotic', 'circular', 'dreamy', 'episodic', 'family', 'identity', 'isolation', 'loss', 'mysterious', 'non-linear', 'surreal', 'transformation')\n"
          ]
        }
      ],
      "source": [
        "# Test run\n",
        "my_review = \"The movie was a bit strange and surreal. It didn't have much of a plot, but the visuals were stunning and it felt like a dream.\"\n",
        "\n",
        "# Get the optimized predictions\n",
        "final_labels = predict_with_optimal_thresholds(review_text=my_review, model_path=BEST_MODEL_PATH)\n",
        "\n",
        "print(f\"Review: '{my_review}'\")\n",
        "print(f\"Predicted Labels: {final_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRHsGOIQh0jn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import joblib\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, SequentialSampler # Good for batching\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "FINAL_MODEL_DIR = BEST_MODEL_PATH\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def classify_reviews_batch_custom(reviews, model_path, batch_size = 32):\n",
        "    print(f\"Using Custom Fine-Tuned Model for Batch Classification\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model directory not found: {model_path}\")\n",
        "\n",
        "    # Load all artifacts ONCE\n",
        "    print(\"Loading model and artifacts...\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model_info = joblib.load(os.path.join(model_path, 'model_info.pkl'))\n",
        "    mlb = model_info['label_binarizer']\n",
        "    optimal_thresholds = model_info['optimal_thresholds']\n",
        "\n",
        "    # Prepare model and thresholds for inference\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    thresholds_tensor = torch.tensor([optimal_thresholds[label] for label in mlb.classes_]).to(device)\n",
        "\n",
        "    # Process reviews in mini-batches\n",
        "    all_predicted_labels = []\n",
        "    print(f\"Starting classification of {len(reviews)} reviews with batch size {batch_size}...\")\n",
        "\n",
        "    for i in range(0, len(reviews), batch_size):\n",
        "        batch_reviews = reviews[i:i+batch_size]\n",
        "        print(f\"  Processing batch {i//batch_size + 1}...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Tokenize the entire batch at once. `padding=True` handles different lengths.\n",
        "            encoding = tokenizer(\n",
        "                batch_reviews,\n",
        "                truncation=True,\n",
        "                padding=True, # Use dynamic padding for batches\n",
        "                max_length=256,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Send the whole batch to the GPU\n",
        "            input_ids = encoding['input_ids'].to(device)\n",
        "            attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "            # Get model outputs for the batch\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probabilities = torch.sigmoid(outputs.logits)\n",
        "\n",
        "            # Apply thresholds to the entire batch\n",
        "            predictions = (probabilities > thresholds_tensor).cpu().numpy().astype(int)\n",
        "\n",
        "            # Decode the batch of predictions back into label names\n",
        "            batch_labels = mlb.inverse_transform(predictions)\n",
        "            all_predicted_labels.extend(batch_labels)\n",
        "\n",
        "    print(\"Batch classification complete.\")\n",
        "    return all_predicted_labels\n",
        "\n",
        "def separate_labels_by_category(all_labels):\n",
        "    atmosphere_results = []\n",
        "    theme_results = []\n",
        "    narrative_results = []\n",
        "\n",
        "    for labels in all_labels:\n",
        "        label_set = set(labels) if labels else set()\n",
        "\n",
        "        atmosphere = [label for label in labels if label in atmosphere_labels] if labels else []\n",
        "        themes = [label for label in labels if label in theme_candidates] if labels else []\n",
        "        narrative = [label for label in labels if label in narrative_labels] if labels else []\n",
        "\n",
        "        atmosphere_results.append(atmosphere)\n",
        "        theme_results.append(themes)\n",
        "        narrative_results.append(narrative)\n",
        "\n",
        "    return atmosphere_results, theme_results, narrative_results\n",
        "\n",
        "def create_classified_embeddings(df, model_path: str, batch_size: int = 32):\n",
        "    df['review'] = df['review'].fillna('')\n",
        "\n",
        "    # Load SentenceTransformer for embeddings\n",
        "    sbert_model = SentenceTransformer(SBERT_MODEL)\n",
        "\n",
        "    print(\"Starting classification with custom model...\")\n",
        "\n",
        "    # Filter out empty reviews for classification\n",
        "    valid_reviews = [(i, review) for i, review in enumerate(df['review'].tolist()) if review.strip()]\n",
        "\n",
        "    if not valid_reviews:\n",
        "        print(\"No valid reviews found for classification.\")\n",
        "        # Create empty results\n",
        "        all_labels = [[] for _ in range(len(df))]\n",
        "    else:\n",
        "        # Extract just the review texts for classification\n",
        "        review_texts = [review for _, review in valid_reviews]\n",
        "\n",
        "        # Classify using the custom model\n",
        "        classified_labels = classify_reviews_batch_custom(review_texts, model_path, batch_size)\n",
        "\n",
        "        # Map results back to original indices\n",
        "        results = {}\n",
        "        for j, (orig_idx, _) in enumerate(valid_reviews):\n",
        "            results[orig_idx] = list(classified_labels[j]) if classified_labels[j] else []\n",
        "\n",
        "        # Create final list with all indices\n",
        "        all_labels = [results.get(i, []) for i in range(len(df))]\n",
        "\n",
        "    print(f\"Completed classification. Processing {len([labels for labels in all_labels if labels])} non-empty classifications.\")\n",
        "\n",
        "    # Separate labels by category\n",
        "    print(\"Separating labels by category...\")\n",
        "    atmosphere_labels, theme_labels, narrative_labels = separate_labels_by_category(all_labels)\n",
        "\n",
        "    def encode_labels(labels):\n",
        "        \"\"\"Encode a list of labels into a single embedding vector.\"\"\"\n",
        "        if not labels:\n",
        "            return np.zeros(384, dtype='float32')\n",
        "        label_embeds = sbert_model.encode(labels, batch_size=32, show_progress_bar=False)\n",
        "        return np.mean(label_embeds, axis=0).astype('float32')\n",
        "\n",
        "    print(\"Encoding atmosphere labels...\")\n",
        "    atmosphere_emb = np.array([encode_labels(labels) for labels in atmosphere_labels])\n",
        "\n",
        "    print(\"Encoding narrative_structure labels...\")\n",
        "    narrative_emb = np.array([encode_labels(labels) for labels in narrative_labels])\n",
        "\n",
        "    print(\"Encoding themes labels...\")\n",
        "    themes_emb = np.array([encode_labels(labels) for labels in theme_labels])\n",
        "\n",
        "    print(\"Encoding combined labels...\")\n",
        "    combined_emb = np.array([encode_labels(labels) for labels in all_labels])\n",
        "\n",
        "    print(\"Completed label encoding.\")\n",
        "\n",
        "    # Create final DataFrame\n",
        "    final_df = pd.DataFrame({\n",
        "        'id': df['id'],\n",
        "        'atmosphere': atmosphere_labels,\n",
        "        'narrative_structure': narrative_labels,\n",
        "        'themes': theme_labels,\n",
        "        'atmosphere_emb': list(atmosphere_emb),\n",
        "        'narrative_emb': list(narrative_emb),\n",
        "        'themes_emb': list(themes_emb),\n",
        "        'combined_emb': list(combined_emb)\n",
        "    })\n",
        "\n",
        "    print(\"Sample results:\")\n",
        "    print(final_df.head())\n",
        "\n",
        "    non_empty_indices = [i for i, labels in enumerate(all_labels) if labels]\n",
        "    if non_empty_indices:\n",
        "        print(f\"\\nExample classifications:\")\n",
        "        for i, idx in enumerate(non_empty_indices[:5]):\n",
        "            print(f\"Example {i+1}:\")\n",
        "            print(f\"Atmosphere: {atmosphere_labels[idx]}\")\n",
        "            print(f\"Themes: {theme_labels[idx]}\")\n",
        "            print(f\"Narrative: {narrative_labels[idx]}\")\n",
        "            print(f\"All labels: {all_labels[idx]}\")\n",
        "\n",
        "    output_path = CLASSIFIED_EMBEDDINGS_PATH\n",
        "    final_df.to_pickle(output_path)\n",
        "    print(f\"\\nSaved results to: {output_path}\")\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def test_create_classified_embeddings_custom(df):\n",
        "    test_df = df[df['title'] == 'Lost Highway'].copy()\n",
        "    print(test_df)\n",
        "    if test_df.empty:\n",
        "        raise ValueError(\"Movie Lost Highway not found in the dataframe\")\n",
        "\n",
        "    print(f\"Testing with movie ID: {test_df['id'].iloc[0]}\")\n",
        "    test_df['review'] = test_df['review'].fillna('')\n",
        "    reviews_to_classify = test_df['review'].tolist()\n",
        "\n",
        "    # Single classification call\n",
        "    all_labels = classify_reviews_batch_custom(reviews_to_classify, FINAL_MODEL_DIR)\n",
        "\n",
        "    print(\"\\nFiltering results into categories...\")\n",
        "    test_df['atmosphere'] = [[label for label in review_labels if label in atmosphere_labels] for review_labels in all_labels]\n",
        "    test_df['narrative_structure'] = [[label for label in review_labels if label in narrative_labels] for review_labels in all_labels]\n",
        "    test_df['themes'] = [[label for label in review_labels if label in theme_candidates] for review_labels in all_labels]\n",
        "\n",
        "    print(f\"Atmosphere results: {test_df['atmosphere'].iloc[0]}\")\n",
        "    print(f\"Narrative structure results: {test_df['narrative_structure'].iloc[0]}\")\n",
        "    print(f\"Themes results: {test_df['themes'].iloc[0]}\")\n",
        "\n",
        "    # Embedding\n",
        "    print(\"\\nStarting label encoding...\")\n",
        "    sbert_model = SentenceTransformer(SBERT_MODEL)\n",
        "\n",
        "    def encode_labels(labels):\n",
        "        if not labels:\n",
        "            return np.zeros(384, dtype='float32')\n",
        "        label_embeds = sbert_model.encode(labels, batch_size=32, show_progress_bar=False)\n",
        "        return np.mean(label_embeds, axis=0).astype('float32')\n",
        "\n",
        "    atmosphere_emb = np.array([encode_labels(labels) for labels in test_df['atmosphere']])\n",
        "    narrative_emb = np.array([encode_labels(labels) for labels in test_df['narrative_structure']])\n",
        "    themes_emb = np.array([encode_labels(labels) for labels in test_df['themes']])\n",
        "\n",
        "    print(\"Completed label encoding.\")\n",
        "\n",
        "    final_df = pd.DataFrame({\n",
        "        'id': test_df['id'],\n",
        "        'atmosphere_emb': list(atmosphere_emb),\n",
        "        'narrative_emb': list(narrative_emb),\n",
        "        'themes_emb': list(themes_emb),\n",
        "    })\n",
        "\n",
        "    print(\"\\nFinal test results with custom model:\")\n",
        "    print(final_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSzN4YDpkvEr",
        "outputId": "2ae6c2c1-cc49-47b1-d8e4-0bab112734a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id                title                                           overview  \\\n",
            "0   2                Ariel  A Finnish man goes to the city to find a job a...   \n",
            "1   3  Shadows in Paradise  Nikander, a rubbish collector and would-be ent...   \n",
            "2   5           Four Rooms  It's Ted the Bellhop's first night on the job....   \n",
            "3   6       Judgment Night  Four young friends, while taking a shortcut en...   \n",
            "4  11            Star Wars  Princess Leia is captured and held hostage by ...   \n",
            "\n",
            "   vote_average                            genres  \\\n",
            "0         7.100        Comedy,Crime,Drama,Romance   \n",
            "1         7.291              Comedy,Drama,Romance   \n",
            "2         5.864                            Comedy   \n",
            "3         6.455             Action,Crime,Thriller   \n",
            "4         8.206  Action,Adventure,Science Fiction   \n",
            "\n",
            "                                            keywords  \\\n",
            "0  factory worker,falling in love,helsinki, finla...   \n",
            "1               garbage,helsinki, finland,salesclerk   \n",
            "2  anthology,bet,hoodlum,hotel,hotel room,los ang...   \n",
            "3  boxing,chicago, illinois,drug dealer,escape,on...   \n",
            "4  android,awestruck,desert,empire,excited,freedo...   \n",
            "\n",
            "                                              review  \n",
            "0  Synopsis: A good looking young guy inherits an...  \n",
            "1  A simpleminded garbage man and a misanthropic ...  \n",
            "2  Four different rooms, four different episodes,...  \n",
            "3  I've heard this film dissed by so many, and I ...  \n",
            "4  I can never pick a favorite movie because diff...  \n",
            "Starting classification with custom model...\n",
            "--- Using Custom Fine-Tuned Model for Batch Classification ---\n",
            "Loading model and artifacts...\n",
            "Starting classification of 9943 reviews with batch size 32...\n",
            "  Processing batch 1...\n",
            "  Processing batch 2...\n",
            "  Processing batch 3...\n",
            "  Processing batch 4...\n",
            "  Processing batch 5...\n",
            "  Processing batch 6...\n",
            "  Processing batch 7...\n",
            "  Processing batch 8...\n",
            "  Processing batch 9...\n",
            "  Processing batch 10...\n",
            "  Processing batch 11...\n",
            "  Processing batch 12...\n",
            "  Processing batch 13...\n",
            "  Processing batch 14...\n",
            "  Processing batch 15...\n",
            "  Processing batch 16...\n",
            "  Processing batch 17...\n",
            "  Processing batch 18...\n",
            "  Processing batch 19...\n",
            "  Processing batch 20...\n",
            "  Processing batch 21...\n",
            "  Processing batch 22...\n",
            "  Processing batch 23...\n",
            "  Processing batch 24...\n",
            "  Processing batch 25...\n",
            "  Processing batch 26...\n",
            "  Processing batch 27...\n",
            "  Processing batch 28...\n",
            "  Processing batch 29...\n",
            "  Processing batch 30...\n",
            "  Processing batch 31...\n",
            "  Processing batch 32...\n",
            "  Processing batch 33...\n",
            "  Processing batch 34...\n",
            "  Processing batch 35...\n",
            "  Processing batch 36...\n",
            "  Processing batch 37...\n",
            "  Processing batch 38...\n",
            "  Processing batch 39...\n",
            "  Processing batch 40...\n",
            "  Processing batch 41...\n",
            "  Processing batch 42...\n",
            "  Processing batch 43...\n",
            "  Processing batch 44...\n",
            "  Processing batch 45...\n",
            "  Processing batch 46...\n",
            "  Processing batch 47...\n",
            "  Processing batch 48...\n",
            "  Processing batch 49...\n",
            "  Processing batch 50...\n",
            "  Processing batch 51...\n",
            "  Processing batch 52...\n",
            "  Processing batch 53...\n",
            "  Processing batch 54...\n",
            "  Processing batch 55...\n",
            "  Processing batch 56...\n",
            "  Processing batch 57...\n",
            "  Processing batch 58...\n",
            "  Processing batch 59...\n",
            "  Processing batch 60...\n",
            "  Processing batch 61...\n",
            "  Processing batch 62...\n",
            "  Processing batch 63...\n",
            "  Processing batch 64...\n",
            "  Processing batch 65...\n",
            "  Processing batch 66...\n",
            "  Processing batch 67...\n",
            "  Processing batch 68...\n",
            "  Processing batch 69...\n",
            "  Processing batch 70...\n",
            "  Processing batch 71...\n",
            "  Processing batch 72...\n",
            "  Processing batch 73...\n",
            "  Processing batch 74...\n",
            "  Processing batch 75...\n",
            "  Processing batch 76...\n",
            "  Processing batch 77...\n",
            "  Processing batch 78...\n",
            "  Processing batch 79...\n",
            "  Processing batch 80...\n",
            "  Processing batch 81...\n",
            "  Processing batch 82...\n",
            "  Processing batch 83...\n",
            "  Processing batch 84...\n",
            "  Processing batch 85...\n",
            "  Processing batch 86...\n",
            "  Processing batch 87...\n",
            "  Processing batch 88...\n",
            "  Processing batch 89...\n",
            "  Processing batch 90...\n",
            "  Processing batch 91...\n",
            "  Processing batch 92...\n",
            "  Processing batch 93...\n",
            "  Processing batch 94...\n",
            "  Processing batch 95...\n",
            "  Processing batch 96...\n",
            "  Processing batch 97...\n",
            "  Processing batch 98...\n",
            "  Processing batch 99...\n",
            "  Processing batch 100...\n",
            "  Processing batch 101...\n",
            "  Processing batch 102...\n",
            "  Processing batch 103...\n",
            "  Processing batch 104...\n",
            "  Processing batch 105...\n",
            "  Processing batch 106...\n",
            "  Processing batch 107...\n",
            "  Processing batch 108...\n",
            "  Processing batch 109...\n",
            "  Processing batch 110...\n",
            "  Processing batch 111...\n",
            "  Processing batch 112...\n",
            "  Processing batch 113...\n",
            "  Processing batch 114...\n",
            "  Processing batch 115...\n",
            "  Processing batch 116...\n",
            "  Processing batch 117...\n",
            "  Processing batch 118...\n",
            "  Processing batch 119...\n",
            "  Processing batch 120...\n",
            "  Processing batch 121...\n",
            "  Processing batch 122...\n",
            "  Processing batch 123...\n",
            "  Processing batch 124...\n",
            "  Processing batch 125...\n",
            "  Processing batch 126...\n",
            "  Processing batch 127...\n",
            "  Processing batch 128...\n",
            "  Processing batch 129...\n",
            "  Processing batch 130...\n",
            "  Processing batch 131...\n",
            "  Processing batch 132...\n",
            "  Processing batch 133...\n",
            "  Processing batch 134...\n",
            "  Processing batch 135...\n",
            "  Processing batch 136...\n",
            "  Processing batch 137...\n",
            "  Processing batch 138...\n",
            "  Processing batch 139...\n",
            "  Processing batch 140...\n",
            "  Processing batch 141...\n",
            "  Processing batch 142...\n",
            "  Processing batch 143...\n",
            "  Processing batch 144...\n",
            "  Processing batch 145...\n",
            "  Processing batch 146...\n",
            "  Processing batch 147...\n",
            "  Processing batch 148...\n",
            "  Processing batch 149...\n",
            "  Processing batch 150...\n",
            "  Processing batch 151...\n",
            "  Processing batch 152...\n",
            "  Processing batch 153...\n",
            "  Processing batch 154...\n",
            "  Processing batch 155...\n",
            "  Processing batch 156...\n",
            "  Processing batch 157...\n",
            "  Processing batch 158...\n",
            "  Processing batch 159...\n",
            "  Processing batch 160...\n",
            "  Processing batch 161...\n",
            "  Processing batch 162...\n",
            "  Processing batch 163...\n",
            "  Processing batch 164...\n",
            "  Processing batch 165...\n",
            "  Processing batch 166...\n",
            "  Processing batch 167...\n",
            "  Processing batch 168...\n",
            "  Processing batch 169...\n",
            "  Processing batch 170...\n",
            "  Processing batch 171...\n",
            "  Processing batch 172...\n",
            "  Processing batch 173...\n",
            "  Processing batch 174...\n",
            "  Processing batch 175...\n",
            "  Processing batch 176...\n",
            "  Processing batch 177...\n",
            "  Processing batch 178...\n",
            "  Processing batch 179...\n",
            "  Processing batch 180...\n",
            "  Processing batch 181...\n",
            "  Processing batch 182...\n",
            "  Processing batch 183...\n",
            "  Processing batch 184...\n",
            "  Processing batch 185...\n",
            "  Processing batch 186...\n",
            "  Processing batch 187...\n",
            "  Processing batch 188...\n",
            "  Processing batch 189...\n",
            "  Processing batch 190...\n",
            "  Processing batch 191...\n",
            "  Processing batch 192...\n",
            "  Processing batch 193...\n",
            "  Processing batch 194...\n",
            "  Processing batch 195...\n",
            "  Processing batch 196...\n",
            "  Processing batch 197...\n",
            "  Processing batch 198...\n",
            "  Processing batch 199...\n",
            "  Processing batch 200...\n",
            "  Processing batch 201...\n",
            "  Processing batch 202...\n",
            "  Processing batch 203...\n",
            "  Processing batch 204...\n",
            "  Processing batch 205...\n",
            "  Processing batch 206...\n",
            "  Processing batch 207...\n",
            "  Processing batch 208...\n",
            "  Processing batch 209...\n",
            "  Processing batch 210...\n",
            "  Processing batch 211...\n",
            "  Processing batch 212...\n",
            "  Processing batch 213...\n",
            "  Processing batch 214...\n",
            "  Processing batch 215...\n",
            "  Processing batch 216...\n",
            "  Processing batch 217...\n",
            "  Processing batch 218...\n",
            "  Processing batch 219...\n",
            "  Processing batch 220...\n",
            "  Processing batch 221...\n",
            "  Processing batch 222...\n",
            "  Processing batch 223...\n",
            "  Processing batch 224...\n",
            "  Processing batch 225...\n",
            "  Processing batch 226...\n",
            "  Processing batch 227...\n",
            "  Processing batch 228...\n",
            "  Processing batch 229...\n",
            "  Processing batch 230...\n",
            "  Processing batch 231...\n",
            "  Processing batch 232...\n",
            "  Processing batch 233...\n",
            "  Processing batch 234...\n",
            "  Processing batch 235...\n",
            "  Processing batch 236...\n",
            "  Processing batch 237...\n",
            "  Processing batch 238...\n",
            "  Processing batch 239...\n",
            "  Processing batch 240...\n",
            "  Processing batch 241...\n",
            "  Processing batch 242...\n",
            "  Processing batch 243...\n",
            "  Processing batch 244...\n",
            "  Processing batch 245...\n",
            "  Processing batch 246...\n",
            "  Processing batch 247...\n",
            "  Processing batch 248...\n",
            "  Processing batch 249...\n",
            "  Processing batch 250...\n",
            "  Processing batch 251...\n",
            "  Processing batch 252...\n",
            "  Processing batch 253...\n",
            "  Processing batch 254...\n",
            "  Processing batch 255...\n",
            "  Processing batch 256...\n",
            "  Processing batch 257...\n",
            "  Processing batch 258...\n",
            "  Processing batch 259...\n",
            "  Processing batch 260...\n",
            "  Processing batch 261...\n",
            "  Processing batch 262...\n",
            "  Processing batch 263...\n",
            "  Processing batch 264...\n",
            "  Processing batch 265...\n",
            "  Processing batch 266...\n",
            "  Processing batch 267...\n",
            "  Processing batch 268...\n",
            "  Processing batch 269...\n",
            "  Processing batch 270...\n",
            "  Processing batch 271...\n",
            "  Processing batch 272...\n",
            "  Processing batch 273...\n",
            "  Processing batch 274...\n",
            "  Processing batch 275...\n",
            "  Processing batch 276...\n",
            "  Processing batch 277...\n",
            "  Processing batch 278...\n",
            "  Processing batch 279...\n",
            "  Processing batch 280...\n",
            "  Processing batch 281...\n",
            "  Processing batch 282...\n",
            "  Processing batch 283...\n",
            "  Processing batch 284...\n",
            "  Processing batch 285...\n",
            "  Processing batch 286...\n",
            "  Processing batch 287...\n",
            "  Processing batch 288...\n",
            "  Processing batch 289...\n",
            "  Processing batch 290...\n",
            "  Processing batch 291...\n",
            "  Processing batch 292...\n",
            "  Processing batch 293...\n",
            "  Processing batch 294...\n",
            "  Processing batch 295...\n",
            "  Processing batch 296...\n",
            "  Processing batch 297...\n",
            "  Processing batch 298...\n",
            "  Processing batch 299...\n",
            "  Processing batch 300...\n",
            "  Processing batch 301...\n",
            "  Processing batch 302...\n",
            "  Processing batch 303...\n",
            "  Processing batch 304...\n",
            "  Processing batch 305...\n",
            "  Processing batch 306...\n",
            "  Processing batch 307...\n",
            "  Processing batch 308...\n",
            "  Processing batch 309...\n",
            "  Processing batch 310...\n",
            "  Processing batch 311...\n",
            "Batch classification complete.\n",
            "Completed classification. Processing 9943 non-empty classifications.\n",
            "Separating labels by category...\n",
            "Encoding atmosphere labels...\n",
            "Encoding narrative_structure labels...\n",
            "Encoding themes labels...\n",
            "Encoding combined labels...\n",
            "Completed label encoding.\n",
            "Sample results:\n",
            "   id            atmosphere narrative_structure  \\\n",
            "0   2  [melancholic, tense]            [linear]   \n",
            "1   3             [neutral]  [episodic, linear]   \n",
            "2   5             [chaotic]  [episodic, linear]   \n",
            "3   6                    []            [linear]   \n",
            "4  11   [foreboding, tense]            [linear]   \n",
            "\n",
            "                                              themes  \\\n",
            "0  [family, identity, isolation, loss, love, rede...   \n",
            "1                [family, identity, isolation, love]   \n",
            "2                                           [family]   \n",
            "3                                 [family, identity]   \n",
            "4  [betrayal, corruption, family, freedom, loyalt...   \n",
            "\n",
            "                                      atmosphere_emb  \\\n",
            "0  [0.030515473, 0.056558505, 0.014792115, 0.0664...   \n",
            "1  [-0.05107651, -0.013697355, -0.05514741, 0.000...   \n",
            "2  [-0.031100709, -0.013574571, 0.039399873, 0.00...   \n",
            "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "4  [0.022279628, 0.038633063, -0.00830493, 0.0637...   \n",
            "\n",
            "                                       narrative_emb  \\\n",
            "0  [-0.051207665, 0.038816027, -0.022586687, 0.04...   \n",
            "1  [-0.02335044, 0.015228443, 0.01904865, -0.0002...   \n",
            "2  [-0.02335044, 0.015228443, 0.01904865, -0.0002...   \n",
            "3  [-0.051207665, 0.038816027, -0.022586687, 0.04...   \n",
            "4  [-0.051207665, 0.038816027, -0.022586687, 0.04...   \n",
            "\n",
            "                                          themes_emb  \\\n",
            "0  [-0.072313435, 0.083888195, -0.0011908145, 0.0...   \n",
            "1  [-0.08936623, 0.06697146, 0.003675945, 0.03390...   \n",
            "2  [-0.090103194, 0.07598019, -0.01739463, 0.0018...   \n",
            "3  [-0.09181671, 0.10757985, -0.0062580495, 0.041...   \n",
            "4  [-0.059686843, 0.07491364, -0.015325718, 0.021...   \n",
            "\n",
            "                                        combined_emb  \n",
            "0  [-0.049637068, 0.07391504, -0.00013381914, 0.0...  \n",
            "1  [-0.06503462, 0.040663626, -0.00033519097, 0.0...  \n",
            "2  [-0.041976206, 0.02321563, 0.015025623, 0.0013...  \n",
            "3  [-0.07828036, 0.08465857, -0.011700928, 0.0423...  \n",
            "4  [-0.040529937, 0.06284046, -0.014572329, 0.033...  \n",
            "\n",
            "Example classifications:\n",
            "  Example 1:\n",
            "    Atmosphere: ['melancholic', 'tense']\n",
            "    Themes: ['family', 'identity', 'isolation', 'loss', 'love', 'redemption', 'transformation']\n",
            "    Narrative: ['linear']\n",
            "    All labels: ['family', 'identity', 'isolation', 'linear', 'loss', 'love', 'melancholic', 'redemption', 'tense', 'transformation']\n",
            "  Example 2:\n",
            "    Atmosphere: ['neutral']\n",
            "    Themes: ['family', 'identity', 'isolation', 'love']\n",
            "    Narrative: ['episodic', 'linear']\n",
            "    All labels: ['episodic', 'family', 'identity', 'isolation', 'linear', 'love', 'neutral']\n",
            "  Example 3:\n",
            "    Atmosphere: ['chaotic']\n",
            "    Themes: ['family']\n",
            "    Narrative: ['episodic', 'linear']\n",
            "    All labels: ['chaotic', 'episodic', 'family', 'linear']\n",
            "  Example 4:\n",
            "    Atmosphere: []\n",
            "    Themes: ['family', 'identity']\n",
            "    Narrative: ['linear']\n",
            "    All labels: ['family', 'identity', 'linear']\n",
            "  Example 5:\n",
            "    Atmosphere: ['foreboding', 'tense']\n",
            "    Themes: ['betrayal', 'corruption', 'family', 'freedom', 'loyalty', 'power']\n",
            "    Narrative: ['linear']\n",
            "    All labels: ['betrayal', 'corruption', 'family', 'foreboding', 'freedom', 'linear', 'loyalty', 'power', 'tense']\n",
            "\n",
            "Saved results to: /content/drive/My Drive/movie_similarity/classified_embeddings.pkl\n",
            "   id            atmosphere narrative_structure  \\\n",
            "0   2  [melancholic, tense]            [linear]   \n",
            "1   3             [neutral]  [episodic, linear]   \n",
            "2   5             [chaotic]  [episodic, linear]   \n",
            "3   6                    []            [linear]   \n",
            "4  11   [foreboding, tense]            [linear]   \n",
            "\n",
            "                                              themes  \\\n",
            "0  [family, identity, isolation, loss, love, rede...   \n",
            "1                [family, identity, isolation, love]   \n",
            "2                                           [family]   \n",
            "3                                 [family, identity]   \n",
            "4  [betrayal, corruption, family, freedom, loyalt...   \n",
            "\n",
            "                                      atmosphere_emb  \\\n",
            "0  [0.030515473, 0.056558505, 0.014792115, 0.0664...   \n",
            "1  [-0.05107651, -0.013697355, -0.05514741, 0.000...   \n",
            "2  [-0.031100709, -0.013574571, 0.039399873, 0.00...   \n",
            "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "4  [0.022279628, 0.038633063, -0.00830493, 0.0637...   \n",
            "\n",
            "                                       narrative_emb  \\\n",
            "0  [-0.051207665, 0.038816027, -0.022586687, 0.04...   \n",
            "1  [-0.02335044, 0.015228443, 0.01904865, -0.0002...   \n",
            "2  [-0.02335044, 0.015228443, 0.01904865, -0.0002...   \n",
            "3  [-0.051207665, 0.038816027, -0.022586687, 0.04...   \n",
            "4  [-0.051207665, 0.038816027, -0.022586687, 0.04...   \n",
            "\n",
            "                                          themes_emb  \\\n",
            "0  [-0.072313435, 0.083888195, -0.0011908145, 0.0...   \n",
            "1  [-0.08936623, 0.06697146, 0.003675945, 0.03390...   \n",
            "2  [-0.090103194, 0.07598019, -0.01739463, 0.0018...   \n",
            "3  [-0.09181671, 0.10757985, -0.0062580495, 0.041...   \n",
            "4  [-0.059686843, 0.07491364, -0.015325718, 0.021...   \n",
            "\n",
            "                                        combined_emb  \n",
            "0  [-0.049637068, 0.07391504, -0.00013381914, 0.0...  \n",
            "1  [-0.06503462, 0.040663626, -0.00033519097, 0.0...  \n",
            "2  [-0.041976206, 0.02321563, 0.015025623, 0.0013...  \n",
            "3  [-0.07828036, 0.08465857, -0.011700928, 0.0423...  \n",
            "4  [-0.040529937, 0.06284046, -0.014572329, 0.033...  \n"
          ]
        }
      ],
      "source": [
        "initial_df = pd.read_pickle(INITIAL_DATA_PATH)\n",
        "\n",
        "print(initial_df.head())\n",
        "\n",
        "final_df = create_classified_embeddings(initial_df, BEST_MODEL_PATH)\n",
        "\n",
        "print(final_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPUYi4O1bx8s",
        "outputId": "ff995ae5-1fc9-40de-be5a-9eba1543b513"
      },
      "outputs": [],
      "source": [
        "test_df = final_df[final_df['id'] == 13].copy()\n",
        "print(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkW0HL-elB7N",
        "outputId": "af6da51b-176e-45e7-c5e1-bc5b53dec68a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Step 1: Loading components ---\n",
            "Loading model from checkpoint: /content/drive/My Drive/movie_similarity/roberta_multilabel_improved/checkpoint-600\n",
            "Loading fresh tokenizer from source: FacebookAI/roberta-base\n",
            "\n",
            "--- Step 2: Saving all components to the final directory ---\n",
            "\n",
            "Successfully saved model and tokenizer to: /content/drive/My Drive/movie_similarity/roberta_multilabel_improved\n",
            "\n",
            "--- Step 3: Verifying the contents of the final directory ---\n",
            "total 491729\n",
            "drwx------ 2 root root      4096 Jul 22 17:28 checkpoint-600\n",
            "drwx------ 2 root root      4096 Jul 22 17:39 checkpoint-738\n",
            "-rw------- 1 root root      1917 Jul 22 17:56 config.json\n",
            "-rw------- 1 root root    456318 Jul 22 17:57 merges.txt\n",
            "-rw------- 1 root root      2035 Jul 22 17:44 model_info.pkl\n",
            "-rw------- 1 root root 498695876 Jul 22 17:57 model.safetensors\n",
            "-rw------- 1 root root       280 Jul 22 17:57 special_tokens_map.json\n",
            "-rw------- 1 root root      1246 Jul 22 17:57 tokenizer_config.json\n",
            "-rw------- 1 root root   3558643 Jul 22 17:57 tokenizer.json\n",
            "-rw------- 1 root root      5432 Jul 22 17:44 training_args.bin\n",
            "-rw------- 1 root root    798293 Jul 22 17:57 vocab.json\n",
            "\n",
            "Verification complete. You should see 'vocab.json' and 'merges.txt' in the list above.\n",
            "You can now safely run your 'predict_with_optimal_thresholds' or other functions.\n"
          ]
        }
      ],
      "source": [
        "# **IF FORGOT TO SAVE TOKENIZER**\n",
        "\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "FINAL_SAVE_PATH = BEST_MODEL_PATH\n",
        "\n",
        "BEST_CHECKPOINT_PATH = '/content/drive/My Drive/movie_similarity/roberta_multilabel_improved/checkpoint-600'\n",
        "\n",
        "MODEL_NAME = ZSC_MODEL\n",
        "\n",
        "print(f\"Loading components\")\n",
        "\n",
        "# Check if the checkpoint path exists\n",
        "if not os.path.exists(BEST_CHECKPOINT_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"CRITICAL: The checkpoint path was not found: {BEST_CHECKPOINT_PATH}\\n\"\n",
        "        \"Please check the folder and update the path to the correct checkpoint directory.\"\n",
        "    )\n",
        "\n",
        "# Load the best model from the specified checkpoint\n",
        "print(f\"Loading model from checkpoint: {BEST_CHECKPOINT_PATH}\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(BEST_CHECKPOINT_PATH)\n",
        "\n",
        "# CRITICAL STEP: Load a fresh tokenizer from the original Hugging Face source.\n",
        "# We do this because the tokenizer files are what's missing in your saved folder.\n",
        "print(f\"Loading fresh tokenizer from source: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"\\nSaving all components to the final directory\")\n",
        "\n",
        "# Ensure the final destination directory exists\n",
        "os.makedirs(FINAL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save the model and the tokenizer to the FINAL, clean directory.\n",
        "# This will create all the necessary files, including vocab.json.\n",
        "model.save_pretrained(FINAL_SAVE_PATH)\n",
        "tokenizer.save_pretrained(FINAL_SAVE_PATH)\n",
        "\n",
        "print(f\"\\nSuccessfully saved model and tokenizer to: {FINAL_SAVE_PATH}\")\n",
        "\n",
        "print(\"\\nVerifying the contents of the final directory ---\")\n",
        "# Let's list the files to make sure everything is there now.\n",
        "!ls -l \"{FINAL_SAVE_PATH}\"\n",
        "\n",
        "print(\"\\nVerification complete. You should see 'vocab.json' and 'merges.txt' in the list above.\")\n",
        "print(\"You can now safely run your 'predict_with_optimal_thresholds' or other functions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHFmXxRRRQOM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2548e622e55645f999d6ad74b76d064e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2879d2cb39b1495db465607c06d10991": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bce33c12b0848aeb20b74be5653b5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a7ed2ab0bbf4c6685b334e9a69f2fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d620b7e619d44237a61c8f787f60308b",
            "max": 938,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f4d368abf5547dda3d70f4feee3a456",
            "value": 938
          }
        },
        "5e4df461efa244889d4bb048c7f93272": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4d368abf5547dda3d70f4feee3a456": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f09a5880a174152844d0ea15daa6d25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cdc69cb5974454a8b9c5e1d49e27c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec299e94d0b44b5ba5bdf0a4eecef4d9",
              "IPY_MODEL_5a7ed2ab0bbf4c6685b334e9a69f2fa5",
              "IPY_MODEL_e1a174fa1a154affbecf1ad1d5a7f942"
            ],
            "layout": "IPY_MODEL_5e4df461efa244889d4bb048c7f93272"
          }
        },
        "d620b7e619d44237a61c8f787f60308b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a174fa1a154affbecf1ad1d5a7f942": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f09a5880a174152844d0ea15daa6d25",
            "placeholder": "​",
            "style": "IPY_MODEL_2879d2cb39b1495db465607c06d10991",
            "value": " 938/938 [00:21&lt;00:00, 78.19it/s]"
          }
        },
        "ec299e94d0b44b5ba5bdf0a4eecef4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2548e622e55645f999d6ad74b76d064e",
            "placeholder": "​",
            "style": "IPY_MODEL_2bce33c12b0848aeb20b74be5653b5f2",
            "value": "Batches: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
